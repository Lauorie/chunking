#!/usr/bin/env python3
"""
æ‰¹é‡RAGæ–‡æ¡£å¤„ç†è„šæœ¬

ç»“åˆæ‰¹é‡æ–‡æ¡£å¤„ç†å’ŒRAGæ‰©å±•åŠŸèƒ½ï¼Œä½¿ç”¨è‡ªå®šä¹‰tokenizerå¤„ç†æ–‡æ¡£å¹¶è¾“å‡ºRAGå‹å¥½çš„æ ¼å¼ã€‚
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any
import concurrent.futures
from tqdm import tqdm

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from transformers import PreTrainedTokenizerFast
except ImportError as e:
    print(f"âš  transformersåº“æœªå®‰è£…: {e}")
    PreTrainedTokenizerFast = None

from chunking import (
    AstMarkdownSplitter, 
    setup_logging, 
    timer, 
    metrics, 
    format_chunk_stats,
    ChunkingError
)


class RAGDocumentProcessor(AstMarkdownSplitter):
    """ä¸“ä¸ºRAGåº”ç”¨ä¼˜åŒ–çš„æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, tokenizer_path: str = None, **kwargs):
        # --- Tokenizer Initialization (Robust) ---
        tokenizer_func = None
        # First, check if the Hugging Face tokenizer library is available and a path is provided
        if PreTrainedTokenizerFast is not None and tokenizer_path and os.path.exists(tokenizer_path):
            try:
                hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)
                # Create a lambda that matches the expected signature for our splitter
                tokenizer_func = lambda text: hf_tokenizer.encode(text, add_special_tokens=False)
            except Exception as e:
                # This will be printed from within the worker process if it fails
                print(f"âš  è­¦å‘Š: æ— æ³•åŠ è½½è‡ªå®šä¹‰tokenizer '{tokenizer_path}': {e}ã€‚å°†å°è¯•ä½¿ç”¨tiktokenã€‚")
        
        # If the custom tokenizer failed or wasn't provided, try tiktoken
        if tokenizer_func is None:
            try:
                import tiktoken
                encoding = tiktoken.encoding_for_model("gpt-4o")
                tokenizer_func = encoding.encode
            except (ImportError, FileNotFoundError):
                # This will be printed from within the worker process if it fails
                print("âš  è­¦å‘Š: tiktokenåŠ è½½å¤±è´¥ã€‚å°†å›é€€åˆ°åŸºäºå­—ç¬¦çš„é•¿åº¦è®¡ç®—ï¼Œåˆ†å—å¯èƒ½ä¸å‡†ç¡®ã€‚")
                # Fallback to character-based counting if all else fails
                tokenizer_func = lambda x: list(x)

        # --- Parent Class Initialization ---
        # Build the configuration dictionary for the parent AstMarkdownSplitter
        rag_defaults = {
            'chunk_size': kwargs.get('chunk_size', 800),
            'chunk_overlap': kwargs.get('chunk_overlap', 100),         
            'convert_table_ratio': kwargs.get('convert_table_ratio', 0.2),  
            'enable_first_line_as_title': True,
            'tokenizer': tokenizer_func  # CRITICAL: Pass the successfully loaded tokenizer to the core splitter
        }
        # Allow any further kwargs to override the defaults
        rag_defaults.update(kwargs)
        
        # Initialize the parent class with the correct configuration
        super().__init__(**rag_defaults)
        
        # --- Local Configuration ---
        self._max_chunk_size = rag_defaults['chunk_size']
        # Keep a reference to the tokenizer for local use (e.g., in reporting)
        self._token_counter = tokenizer_func
    
    def process_for_rag(self, text: str, metadata: dict = None) -> List[dict]:
        """å¤„ç†æ–‡æ¡£å¹¶è¿”å›RAGå‹å¥½çš„æ ¼å¼"""
        if metadata:
            metadata_str = self._format_metadata(metadata)
            chunks = self.split_text_metadata_aware(text, metadata_str)
        else:
            chunks = self.split_text(text)
        
        # è¿‡æ»¤å’Œåˆå¹¶è¿‡å°çš„å—
        chunks = self._merge_small_chunks(chunks)
        
        rag_chunks = []
        for i, chunk in enumerate(chunks):
            estimated_tokens = self._count_tokens_safe(chunk)
            rag_chunk = {
                'id': f"chunk_{i:04d}",
                'content': chunk,
                'metadata': metadata or {},
                'chunk_index': i,
                'total_chunks': len(chunks),
                'char_count': len(chunk),
                'estimated_tokens': estimated_tokens,
                'content_type': self._get_chunk_content_tag(chunk),
                'content_hash': hash(chunk) % (10**8),
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            rag_chunks.append(rag_chunk)
        
        return rag_chunks
    
    def _format_metadata(self, metadata: dict) -> str:
        """æ ¼å¼åŒ–å…ƒæ•°æ®ä¸ºå­—ç¬¦ä¸²"""
        return " | ".join([f"{k}: {v}" for k, v in metadata.items()])
    
    def _count_tokens_safe(self, text: str) -> int:
        """å®‰å…¨çš„tokenè®¡æ•°æ–¹æ³•ï¼Œä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨ç¡®ä¿ä¸€è‡´æ€§ã€‚"""
        # ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨çš„æ–¹æ³•ç¡®ä¿ä¸åˆ†å—è¿‡ç¨‹ä¸­çš„tokenè®¡ç®—ä¸€è‡´
        return self._cache_manager.get_token_count(text, self._tokenizer)

    def _merge_small_chunks(self, chunks: List[str], min_tokens: int = None) -> List[str]:
        """åˆå¹¶è¿‡å°çš„å—ä»¥æé«˜RAGè´¨é‡"""
        if not chunks:
            return chunks
        
        # ä½¿ç”¨é…ç½®çš„æœ€å°å—å¤§å°ï¼Œé»˜è®¤50
        if min_tokens is None:
            min_tokens = getattr(self, '_min_chunk_tokens', 50)
        
        merged_chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for i, chunk in enumerate(chunks):
            chunk = chunk.strip()
            if not chunk:  # è·³è¿‡ç©ºå—
                continue
                
            chunk_tokens = self._count_tokens_safe(chunk)
            
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœå½“å‰å—æ˜¯å°æ ‡é¢˜ï¼ˆå¦‚"# å‚è€ƒæ–‡æ¡£"ï¼‰ï¼Œå°è¯•ä¸ä¸‹ä¸€å—åˆå¹¶
            is_small_heading = (chunk_tokens < min_tokens and 
                              chunk.startswith('#') and 
                              i < len(chunks) - 1)  # ä¸æ˜¯æœ€åä¸€å—
            
            # å¦‚æœå½“å‰å—å·²ç»è¶³å¤Ÿå¤§ï¼Œæˆ–è€…åŠ ä¸Šæ–°å—ä¼šè¶…è¿‡æœ€å¤§å¤§å°ï¼ˆé™¤éæ˜¯å°æ ‡é¢˜ï¼‰
            if (current_tokens >= min_tokens and not is_small_heading) or \
               (current_tokens + chunk_tokens > self._max_chunk_size and current_chunk and not is_small_heading):
                if current_chunk:
                    merged_chunks.append(current_chunk.strip())
                current_chunk = chunk
                current_tokens = chunk_tokens
            else:
                # åˆå¹¶åˆ°å½“å‰å—
                if current_chunk:
                    current_chunk += "\n\n" + chunk
                    current_tokens = self._count_tokens_safe(current_chunk)
                else:
                    current_chunk = chunk
                    current_tokens = chunk_tokens
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk:
            merged_chunks.append(current_chunk.strip())
        
        # åå¤„ç†ï¼šå¦‚æœæœ€åä¸€ä¸ªå—å¤ªå°ï¼Œå°è¯•ä¸å€’æ•°ç¬¬äºŒä¸ªå—åˆå¹¶
        if len(merged_chunks) >= 2:
            last_chunk = merged_chunks[-1]
            second_last_chunk = merged_chunks[-2]
            
            last_chunk_tokens = self._count_tokens_safe(last_chunk)
            second_last_tokens = self._count_tokens_safe(second_last_chunk)
            
            # å¦‚æœæœ€åä¸€ä¸ªå—å¤ªå°ï¼Œä¸”å€’æ•°ç¬¬äºŒä¸ªå—ä¸è¶…è¿‡CHUNK_SIZE
            if (last_chunk_tokens < min_tokens and 
                second_last_tokens <= self._max_chunk_size):
                
                # å°è¯•åˆå¹¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¼šè¶…è¿‡CHUNK_SIZE
                combined_chunk = second_last_chunk + "\n\n" + last_chunk
                combined_tokens = self._count_tokens_safe(combined_chunk)
                
                if combined_tokens <= self._max_chunk_size:
                    # æ‰§è¡Œåˆå¹¶ï¼šç§»é™¤æœ€åä¸¤ä¸ªå—ï¼Œæ·»åŠ åˆå¹¶åçš„å—
                    merged_chunks = merged_chunks[:-2] + [combined_chunk]
                    print(f"  ğŸ’¡ æœ€åå—åˆå¹¶: {last_chunk_tokens} + {second_last_tokens} = {combined_tokens} tokens")
        
        return merged_chunks

    def _get_chunk_content_tag(self, chunk: str) -> str:
        """åˆ†æå—å†…å®¹ç±»å‹"""
        chunk_lines = chunk.strip().split('\n')
        first_line = chunk_lines[0] if chunk_lines else ""
        
        if '|' in chunk and '---' in chunk:
            return 'table'
        elif first_line.startswith('#'):
            level = len(first_line) - len(first_line.lstrip('#'))
            return f'heading_h{level}'
        elif any(line.strip().startswith(('- ', '* ', '+ ')) for line in chunk_lines):
            return 'unordered_list'
        elif any(line.strip().split('.')[0].isdigit() for line in chunk_lines if '.' in line):
            return 'ordered_list'
        elif '```' in chunk:
            return 'code_block'
        elif any(line.strip().startswith('>') for line in chunk_lines):
            return 'blockquote'
        else:
            return 'paragraph'

    # ç»§æ‰¿å¹¶ç¡®ä¿çˆ¶ç±»æ–¹æ³•å¯ç”¨
    def _process_block_for_splitting(self, child, state, max_chunk_size):
        """é‡è½½çˆ¶ç±»æ–¹æ³•ä»¥ç¡®ä¿å…¼å®¹æ€§"""
        return super()._process_block_for_splitting(child, state, max_chunk_size)
    
    def _handle_block_fits_current_chunk(self, block_content, block_size, state, max_chunk_size):
        """é‡è½½çˆ¶ç±»æ–¹æ³•ä»¥ç¡®ä¿å…¼å®¹æ€§"""
        return super()._handle_block_fits_current_chunk(block_content, block_size, state, max_chunk_size)
    
    def _handle_block_fits_next_chunk(self, child, block_size, state, children_to_process, max_chunk_size):
        """é‡è½½çˆ¶ç±»æ–¹æ³•ä»¥ç¡®ä¿å…¼å®¹æ€§"""
        return super()._handle_block_fits_next_chunk(child, block_size, state, children_to_process, max_chunk_size)

def extract_file_metadata(file_path: Path) -> Dict[str, Any]:
    """ä»æ–‡ä»¶è·¯å¾„å’Œå†…å®¹ä¸­æå–å…ƒæ•°æ®"""
    stat = file_path.stat()
    
    metadata = {
        'filename': file_path.name,
        'filepath': str(file_path),
        'file_size': stat.st_size,
        'created_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_ctime)),
        'modified_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime)),
        'file_extension': file_path.suffix.lower()
    }
    
    # å°è¯•ä»æ–‡ä»¶å†…å®¹æå–æ›´å¤šå…ƒæ•°æ®
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # ç»Ÿè®¡ä¿¡æ¯
        metadata.update({
            'char_count': len(content),
            'line_count': content.count('\n') + 1,
            'word_count': len(content.split()),
            'paragraph_count': len([p for p in content.split('\n\n') if p.strip()]),
            'heading_count': content.count('#'),
            'table_count': content.count('|'),
            'code_block_count': content.count('```') // 2
        })
        
        # æå–ç¬¬ä¸€è¡Œä½œä¸ºå¯èƒ½çš„æ ‡é¢˜
        first_line = content.split('\n')[0].strip()
        if first_line:
            if first_line.startswith('#'):
                metadata['document_title'] = first_line.lstrip('#').strip()
            else:
                metadata['document_title'] = first_line[:100]  # æˆªå–å‰100å­—ç¬¦
                
    except Exception as e:
        print(f"âš  æå–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥ {file_path.name}: {e}")
    
    return metadata


def save_rag_chunks_to_json(rag_chunks: List[dict], output_file: Path, source_metadata: dict):
    """ä¿å­˜RAGå—åˆ°JSONæ ¼å¼"""
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
    total_tokens = sum(chunk['estimated_tokens'] for chunk in rag_chunks)
    content_types = {}
    for chunk in rag_chunks:
        content_type = chunk['content_type']
        content_types[content_type] = content_types.get(content_type, 0) + 1
    
    result = {
        'document_info': {
            'source_file': source_metadata['filename'],
            'source_path': source_metadata['filepath'],
            'processed_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'processor_version': '1.0.0'
        },
        'source_metadata': source_metadata,
        'chunking_summary': {
            'total_chunks': len(rag_chunks),
            'total_estimated_tokens': total_tokens,
            'avg_tokens_per_chunk': total_tokens / len(rag_chunks) if rag_chunks else 0,
            'content_type_distribution': content_types,
            'chunk_size_stats': {
                'min_chars': min(chunk['char_count'] for chunk in rag_chunks) if rag_chunks else 0,
                'max_chars': max(chunk['char_count'] for chunk in rag_chunks) if rag_chunks else 0,
                'avg_chars': sum(chunk['char_count'] for chunk in rag_chunks) / len(rag_chunks) if rag_chunks else 0
            }
        },
        'chunks': rag_chunks
    }
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def process_file_worker(doc_file_path_str: str, rag_processor_config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä¸€ä¸ªç‹¬ç«‹çš„workerå‡½æ•°ï¼Œç”¨äºåœ¨å­è¿›ç¨‹ä¸­å¤„ç†å•ä¸ªæ–‡ä»¶ã€‚
    æ­¤å‡½æ•°ç°åœ¨åŒ…å«å®Œæ•´çš„é”™è¯¯æ•è·å’Œè¿½æº¯åŠŸèƒ½ã€‚
    """
    try:
        doc_file = Path(doc_file_path_str)
        
        # åœ¨workerå†…éƒ¨åˆå§‹åŒ–å¤„ç†å™¨ï¼Œè¿™æ˜¯å¹¶è¡Œå¤„ç†çš„å…³é”®
        rag_processor = RAGDocumentProcessor(
            tokenizer_path=rag_processor_config.get("tokenizer_path"),
            chunk_size=rag_processor_config.get("chunk_size"),
            chunk_overlap=rag_processor_config.get("chunk_overlap"),
            convert_table_ratio=rag_processor_config.get("convert_table_ratio")
        )
        # è®¾ç½®æœ€å°å—å¤§å°
        rag_processor._min_chunk_tokens = rag_processor_config.get("min_chunk_tokens", 50)
        
        output_dir = Path(rag_processor_config["output_dir"])

        with open(doc_file, 'r', encoding='utf-8') as f:
            content = f.read()

        if not content.strip():
            return {'status': 'skipped', 'file': doc_file.name, 'reason': 'ç©ºæ–‡ä»¶'}

        file_metadata = extract_file_metadata(doc_file)
        rag_chunks = rag_processor.process_for_rag(content, file_metadata)

        if not rag_chunks:
            return {'status': 'skipped', 'file': doc_file.name, 'reason': 'æœªç”Ÿæˆä»»ä½•å—'}

        output_file = output_dir / f"{doc_file.stem}_rag_chunks.json"
        save_rag_chunks_to_json(rag_chunks, output_file, file_metadata)

        chunk_stats_str = format_chunk_stats([chunk['content'] for chunk in rag_chunks])
        token_count = sum(chunk['estimated_tokens'] for chunk in rag_chunks)

        return {
            'status': 'success',
            'file': doc_file.name,
            'chunk_count': len(rag_chunks),
            'token_count': token_count,
            'stats_str': f"{chunk_stats_str}, Tokens: {token_count}"
        }

    except Exception as e:
        import traceback
        # æ•è·å¹¶è¿”å›å®Œæ•´çš„é”™è¯¯ä¿¡æ¯å’Œå †æ ˆï¼Œä»¥ä¾¿äºä¸»è¿›ç¨‹è¯Šæ–­é—®é¢˜
        error_str = f"å¤„ç† {Path(doc_file_path_str).name} æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}\n{traceback.format_exc()}"
        # ä¹Ÿåœ¨workerè¿›ç¨‹ä¸­æ‰“å°é”™è¯¯ï¼Œä¾¿äºè°ƒè¯•
        print(f"[WORKER ERROR] {error_str}")
        return {'status': 'failure', 'file': Path(doc_file_path_str).name, 'error': error_str}

def process_document_batch_rag(
    data_dir: str, 
    output_dir: str = None,  # æ–°å¢è¾“å‡ºç›®å½•å‚æ•°
    tokenizer_path: str = None,
    chunk_size: int = 512,
    chunk_overlap: int = 50,
    convert_table_ratio: float = 0.3,
    min_chunk_tokens: int = 50
):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£å¹¶ç”ŸæˆRAGå‹å¥½çš„è¾“å‡º"""
    
    data_path = Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = data_path / "rag_chunks"  # é»˜è®¤è¾“å‡ºç›®å½•
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å°†RAGå¤„ç†å™¨é…ç½®æ‰“åŒ…ï¼Œä»¥ä¾¿ä¼ é€’ç»™worker
    rag_processor_config = {
        "tokenizer_path": tokenizer_path,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "convert_table_ratio": convert_table_ratio,
        "min_chunk_tokens": min_chunk_tokens,
        "output_dir": str(output_path)  # ä½¿ç”¨æ–°çš„è¾“å‡ºç›®å½•
    }
    
    # é‡ç½®æ€§èƒ½æŒ‡æ ‡
    metrics.reset()
    
    # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
    supported_extensions = ['.md', '.txt', '.markdown', '.rst']
    doc_files = []
    for ext in supported_extensions:
        doc_files.extend(data_path.glob(f"**/*{ext}"))
    
    if not doc_files:
        print(f"âš  åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶ ({', '.join(supported_extensions)})")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    print(f"ğŸ”§ é…ç½®: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}, convert_table_ratio={convert_table_ratio}")
    print(f"ğŸ“ æœ€å°å—å¤§å°: {min_chunk_tokens} tokens (ä½äºæ­¤å€¼çš„å—å°†è¢«åˆå¹¶)")
    print(f"ğŸ“Š tokenizer: {'è‡ªå®šä¹‰' if tokenizer_path else 'é»˜è®¤'}")
    # é™åˆ¶å¹¶è¡Œåº¦ä¸º32ï¼Œå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
    max_workers = min(32, os.cpu_count() or 1)
    print(f"âš™ï¸  å¹¶è¡Œæ•°: {max_workers} ä¸ªè¿›ç¨‹ (é™åˆ¶ä¸º32)")
    print("-" * 60)
    
    processed_count = 0
    failed_count = 0
    skipped_count = 0
    total_chunks = 0
    
    with timer(f"å¹¶è¡ŒRAGå¤„ç† {len(doc_files)} ä¸ªæ–‡æ¡£"), concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        # åˆ›å»ºfuturesåˆ—è¡¨ï¼Œå°†ä»»åŠ¡æäº¤ç»™è¿›ç¨‹æ± 
        futures = [executor.submit(process_file_worker, str(doc_file), rag_processor_config) for doc_file in doc_files]

        # ä½¿ç”¨tqdmæ¥æ˜¾ç¤ºè¿›åº¦æ¡
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(doc_files), desc="å¤„ç†æ–‡æ¡£"):
            try:
                result = future.result()
                if result['status'] == 'success':
                    # print(f"âœ“ {result['file']}: {result['stats_str']}") # è¿›åº¦æ¡å­˜åœ¨æ—¶ï¼Œè¿™è¡Œå¯ä»¥æ³¨é‡Šæ‰ä»¥ä¿æŒè¾“å‡ºæ•´æ´
                    processed_count += 1
                    total_chunks += result['chunk_count']
                elif result['status'] == 'skipped':
                    print(f"âš  {result['file']}: è·³è¿‡ ({result['reason']})")
                    skipped_count += 1
                else: # failure
                    print(f"âœ— {result['file']}: å¤±è´¥ ({result['error']})")
                    failed_count += 1
            except Exception as e:
                # future.result() æœ¬èº«ä¹Ÿå¯èƒ½æŠ›å‡ºå¼‚å¸¸
                print(f"âœ— ä¸€ä¸ªworkerè¿›ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                failed_count += 1

    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    print("-" * 60)
    print("ğŸ“ˆ æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"  âœ“ æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ–‡ä»¶")
    print(f"  âš  å¤„ç†è·³è¿‡: {skipped_count} ä¸ªæ–‡ä»¶")
    print(f"  âœ— å¤„ç†å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
    print(f"  ğŸ“¦ æ€»ç”Ÿæˆå—æ•°: {total_chunks}")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    
    # æ€§èƒ½æŒ‡æ ‡
    # æ³¨æ„ï¼šmetricsæ¨¡å—åœ¨å­è¿›ç¨‹ä¸­æ›´æ–°çš„æ•°æ®ä¸ä¼šåæ˜ åœ¨ä¸»è¿›ç¨‹ä¸­ã€‚
    # è¿™é‡Œçš„æ€§èƒ½æŒ‡æ ‡åªåæ˜ äº†ä»»åŠ¡æäº¤å’Œç»“æœæ”¶é›†çš„æ—¶é—´ï¼Œè€Œä¸æ˜¯å®é™…çš„æ–‡ä»¶å¤„ç†æ—¶é—´ã€‚
    # æ›´å‡†ç¡®çš„æ€§èƒ½åˆ†æåº”åŸºäºtqdmçš„é€Ÿç‡æˆ–å¤„ç†æŠ¥å‘Šä¸­çš„æ€»è€—æ—¶ã€‚
    print(f"  â± æ€»ä½“å¤„ç†è€—æ—¶ï¼ˆåŒ…æ‹¬ä»»åŠ¡è°ƒåº¦ï¼‰åœ¨ä¸Šé¢çš„ 'timer' ä¸­æ˜¾ç¤ºã€‚")
    
    # æ”¶é›†æ‰€æœ‰å—çš„tokenç»Ÿè®¡
    all_chunk_tokens = []
    all_chunk_chars = []
    oversized_chunks = []
    file_chunk_stats = {}
    
    # é‡æ–°æ‰«ææ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ä»¥æ”¶é›†tokenç»Ÿè®¡
    for json_file in output_path.glob("*_rag_chunks.json"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                chunks = data.get('chunks', [])
                
                file_tokens = []
                file_chars = []
                file_oversized = []
                
                for chunk in chunks:
                    token_count = chunk.get('estimated_tokens', 0)
                    char_count = chunk.get('char_count', 0)
                    
                    all_chunk_tokens.append(token_count)
                    all_chunk_chars.append(char_count)
                    file_tokens.append(token_count)
                    file_chars.append(char_count)
                    
                    # æ£€æŸ¥è¶…é•¿å—ï¼ˆè¶…è¿‡é…ç½®chunk_sizeçš„120%ï¼‰
                    if token_count > chunk_size * 1.2:
                        oversized_info = {
                            'file': json_file.stem,
                            'chunk_id': chunk.get('chunk_id', 'unknown'),
                            'token_count': token_count,
                            'char_count': char_count,
                            'content_preview': chunk.get('content', '')[:100] + '...'
                        }
                        oversized_chunks.append(oversized_info)
                        file_oversized.append(oversized_info)
                
                # æ–‡ä»¶çº§åˆ«ç»Ÿè®¡
                if file_tokens:
                    file_chunk_stats[json_file.stem] = {
                        'chunk_count': len(file_tokens),
                        'token_stats': {
                            'min': min(file_tokens),
                            'max': max(file_tokens),
                            'avg': sum(file_tokens) / len(file_tokens),
                            'total': sum(file_tokens)
                        },
                        'char_stats': {
                            'min': min(file_chars),
                            'max': max(file_chars),
                            'avg': sum(file_chars) / len(file_chars),
                            'total': sum(file_chars)
                        },
                        'oversized_chunks': len(file_oversized),
                        'oversized_details': file_oversized
                    }
        except Exception as e:
            print(f"âš  æ— æ³•è¯»å–ç»Ÿè®¡æ–‡ä»¶ {json_file}: {e}")
    
    # è®¡ç®—å…¨å±€tokenç»Ÿè®¡
    token_statistics = {}
    if all_chunk_tokens:
        token_statistics = {
            'total_chunks': len(all_chunk_tokens),
            'token_distribution': {
                'min': min(all_chunk_tokens),
                'max': max(all_chunk_tokens),
                'avg': sum(all_chunk_tokens) / len(all_chunk_tokens),
                'median': sorted(all_chunk_tokens)[len(all_chunk_tokens)//2],
                'total': sum(all_chunk_tokens)
            },
            'char_distribution': {
                'min': min(all_chunk_chars),
                'max': max(all_chunk_chars),
                'avg': sum(all_chunk_chars) / len(all_chunk_chars),
                'median': sorted(all_chunk_chars)[len(all_chunk_chars)//2],
                'total': sum(all_chunk_chars)
            },
            'chunk_size_analysis': {
                'within_limit': sum(1 for t in all_chunk_tokens if t <= chunk_size),
                'slightly_over': sum(1 for t in all_chunk_tokens if chunk_size < t <= chunk_size * 1.2),
                'significantly_over': sum(1 for t in all_chunk_tokens if t > chunk_size * 1.2),
                'oversized_percentage': (sum(1 for t in all_chunk_tokens if t > chunk_size * 1.2) / len(all_chunk_tokens)) * 100,
                'too_small': sum(1 for t in all_chunk_tokens if t < min_chunk_tokens),
                'too_small_percentage': (sum(1 for t in all_chunk_tokens if t < min_chunk_tokens) / len(all_chunk_tokens)) * 100
            },
            'quality_indicators': {
                'avg_utilization': (sum(all_chunk_tokens) / len(all_chunk_tokens)) / chunk_size * 100,
                'size_consistency': 100 - (max(all_chunk_tokens) - min(all_chunk_tokens)) / chunk_size * 100 if chunk_size > 0 else 0,
                'oversized_alert': len(oversized_chunks) > 0
            }
        }

    # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
    report_file = output_path / "processing_report.json"
    report = {
        'processing_summary': {
            'total_files': len(doc_files),
            'processed_files': processed_count,
            'failed_files': failed_count,
            'total_chunks_generated': total_chunks,
            'processing_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'configuration': {
                'chunk_size': chunk_size,
                'chunk_overlap': chunk_overlap,
                'convert_table_ratio': convert_table_ratio,
                'min_chunk_tokens': min_chunk_tokens,
                'tokenizer_path': tokenizer_path
            }
        },
        'token_statistics': token_statistics,
        'oversized_chunks_alert': {
            'count': len(oversized_chunks),
            'severity': 'HIGH' if len(oversized_chunks) > total_chunks * 0.1 else 'MEDIUM' if len(oversized_chunks) > 0 else 'NONE',
            'details': oversized_chunks[:10],  # åªæ˜¾ç¤ºå‰10ä¸ªè¶…é•¿å—
            'recommendation': 'Consider reducing chunk_size or improving content splitting logic' if len(oversized_chunks) > 0 else 'Chunk sizes are within acceptable limits'
        },
        'file_level_statistics': file_chunk_stats,
        'performance_metrics': metrics.get_summary(),
        'processed_files': [f.name for f in doc_files[:processed_count]],
        'failed_files': [f.name for f in doc_files[processed_count:processed_count+failed_count]] if failed_count > 0 else []
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"  ğŸ“‹ å¤„ç†æŠ¥å‘Š: {report_file}")
    
    # æ˜¾ç¤ºtokenç»Ÿè®¡æ‘˜è¦
    if token_statistics:
        print("\nğŸ” Tokenç»Ÿè®¡æ‘˜è¦:")
        token_dist = token_statistics['token_distribution']
        chunk_analysis = token_statistics['chunk_size_analysis']
        quality = token_statistics['quality_indicators']
        
        print(f"  ğŸ“Š Tokenåˆ†å¸ƒ: æœ€å°{token_dist['min']}, æœ€å¤§{token_dist['max']}, å¹³å‡{token_dist['avg']:.1f}")
        print(f"  ğŸ“ å—å¤§å°åˆ†æ: æ­£å¸¸{chunk_analysis['within_limit']}, ç•¥è¶…{chunk_analysis['slightly_over']}, ä¸¥é‡è¶…{chunk_analysis['significantly_over']}, è¿‡å°{chunk_analysis['too_small']}")
        print(f"  ğŸ“ˆ è´¨é‡æŒ‡æ ‡: åˆ©ç”¨ç‡{quality['avg_utilization']:.1f}%, ä¸€è‡´æ€§{quality['size_consistency']:.1f}%")
        
        # æ˜¾ç¤ºå°å—åˆå¹¶æ•ˆæœ
        if chunk_analysis['too_small'] > 0:
            print(f"  âš ï¸  å°å—è­¦å‘Š: {chunk_analysis['too_small']}ä¸ªå—å°äº{min_chunk_tokens} tokens ({chunk_analysis['too_small_percentage']:.1f}%)")
            print(f"     å»ºè®®å¢åŠ min_chunk_tokensæˆ–æ£€æŸ¥åˆå¹¶é€»è¾‘")
        else:
            print(f"  âœ… å°å—æ£€æŸ¥: æ‰€æœ‰å—éƒ½ç¬¦åˆæœ€å°å¤§å°è¦æ±‚(â‰¥{min_chunk_tokens} tokens)")
        
        # è¶…é•¿å—è­¦å‘Š
        if len(oversized_chunks) > 0:
            severity = report['oversized_chunks_alert']['severity']
            severity_emoji = "ğŸ”´" if severity == "HIGH" else "ğŸŸ¡" if severity == "MEDIUM" else "ğŸŸ¢"
            print(f"  {severity_emoji} è¶…é•¿å—è­¦å‘Š: {len(oversized_chunks)}ä¸ªå—è¶…è¿‡é™åˆ¶({severity})")
            if len(oversized_chunks) <= 3:
                for chunk in oversized_chunks:
                    print(f"    - {chunk['file']}: {chunk['token_count']} tokens ({chunk['char_count']} chars)")
        else:
            print(f"  âœ… å—å¤§å°æ£€æŸ¥: æ‰€æœ‰å—éƒ½åœ¨å¯æ¥å—èŒƒå›´å†…")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logging(level="INFO")
    
    # é…ç½®å‚æ•°
    DATA_DIR = "/ALL"  # è¾“å…¥ç›®å½•
    OUTPUT_DIR = "/chunks"  # è¾“å‡ºç›®å½•
    TOKENIZER_PATH = "/tokenizer.json"
    
    # å¤„ç†å‚æ•°ï¼ˆä¼˜åŒ–è¶…å¤§è¡¨æ ¼åˆ†å—ï¼‰
    CHUNK_SIZE = 1024        # å‡å°chunk_sizeï¼Œé¿å…è¶…å¤§å—
    CHUNK_OVERLAP = 204      # é€‚ä¸­çš„é‡å 
    CONVERT_TABLE_RATIO = 0.3  # é™ä½é˜ˆå€¼ï¼Œæ›´æ¿€è¿›åœ°è½¬æ¢è¡¨æ ¼ä¸ºæ®µè½
    MIN_CHUNK_TOKENS = 64    # æœ€å°å—å¤§å°ï¼Œä½äºæ­¤å€¼å°†è¢«åˆå¹¶
    
    print("ğŸš€ å¯åŠ¨æ‰¹é‡RAGæ–‡æ¡£å¤„ç†")
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {DATA_DIR}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ”§ Tokenizer: {TOKENIZER_PATH}")
    print(f"âš™ï¸  å‚æ•°: chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}, table_ratio={CONVERT_TABLE_RATIO}, min_tokens={MIN_CHUNK_TOKENS}")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥tokenizeræ–‡ä»¶
        if not os.path.exists(TOKENIZER_PATH):
            print(f"âš  Tokenizeræ–‡ä»¶ä¸å­˜åœ¨: {TOKENIZER_PATH}")
            print("  å°†ä½¿ç”¨é»˜è®¤tokenizer")
            TOKENIZER_PATH = None
        
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        process_document_batch_rag(
            data_dir=DATA_DIR,
            output_dir=OUTPUT_DIR,
            tokenizer_path=TOKENIZER_PATH,
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            convert_table_ratio=CONVERT_TABLE_RATIO,
            min_chunk_tokens=MIN_CHUNK_TOKENS
        )
        
        print("\nğŸ‰ æ‰¹é‡RAGå¤„ç†å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš  ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main() 

# ASTæ™ºèƒ½æ–‡æœ¬åˆ†å—åº“

> ä¸€ä¸ªåŸºäºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆAbstract Syntax Treeï¼ŒASTï¼‰çš„é«˜æ€§èƒ½ Markdown æ–‡æœ¬åˆ†å—åº“ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ–‡æ¡£å¤„ç†å’Œ RAGï¼ˆRetrieval-Augmented Generationï¼‰åº”ç”¨è®¾è®¡ã€‚

[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-1.0.0-orange.svg)](CHANGELOG.md)

---

## ğŸ“š ç›®å½•

- [ğŸ¯ é¡¹ç›®æ¦‚è¿°](#ğŸ¯-é¡¹ç›®æ¦‚è¿°)
  - [æ ¸å¿ƒä»·å€¼](#æ ¸å¿ƒä»·å€¼)
  - [æŠ€æœ¯ç‰¹è‰²](#æŠ€æœ¯ç‰¹è‰²)
  - [ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”](#ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”)
- [ğŸš€ å¿«é€Ÿå¼€å§‹](#ğŸš€-å¿«é€Ÿå¼€å§‹)
  - [å®‰è£…](#å®‰è£…)
  - [5åˆ†é’Ÿä¸Šæ‰‹](#5åˆ†é’Ÿä¸Šæ‰‹)
  - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [ğŸ”¬ ç®—æ³•åŸç†](#ğŸ”¬-ç®—æ³•åŸç†)
  - [ASTè§£ææœºåˆ¶](#1-astè§£ææœºåˆ¶)
  - [æ™ºèƒ½åˆ†å—ç­–ç•¥](#2-æ™ºèƒ½åˆ†å—ç­–ç•¥)
  - [å±‚æ¬¡ç»“æ„ç»´æŠ¤](#3-å±‚æ¬¡ç»“æ„ç»´æŠ¤)
  - [å…ƒæ•°æ®æ„ŸçŸ¥åˆ†å—](#4-å…ƒæ•°æ®æ„ŸçŸ¥åˆ†å—)
- [ğŸ—ï¸ æ¶æ„è®¾è®¡](#ğŸ—ï¸-æ¶æ„è®¾è®¡)
- [âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯](#âš¡-æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯)
- [ğŸ”§ é«˜çº§é…ç½®](#ğŸ”§-é«˜çº§é…ç½®)
- [ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•](#ğŸ“Š-æ€§èƒ½åŸºå‡†æµ‹è¯•)
- [ğŸ§ª å®Œæ•´ä½¿ç”¨ç¤ºä¾‹](#ğŸ§ª-å®Œæ•´ä½¿ç”¨ç¤ºä¾‹)
- [ğŸ¨ å®é™…åº”ç”¨åœºæ™¯](#ğŸ¨-å®é™…åº”ç”¨åœºæ™¯)
- [ğŸ“– APIå‚è€ƒæ–‡æ¡£](#ğŸ“–-apiå‚è€ƒæ–‡æ¡£)
- [ğŸ” æ•…éšœæ’é™¤](#ğŸ”-æ•…éšœæ’é™¤)
- [âš™ï¸ æ€§èƒ½è°ƒä¼˜æŒ‡å—](#âš™ï¸-æ€§èƒ½è°ƒä¼˜æŒ‡å—)
- [â“ å¸¸è§é—®é¢˜FAQ](#â“-å¸¸è§é—®é¢˜faq)
- [ğŸ¤ è´¡çŒ®æŒ‡å—](#ğŸ¤-è´¡çŒ®æŒ‡å—)
- [ğŸ“ˆ æŠ€æœ¯è·¯çº¿å›¾](#ğŸ“ˆ-æŠ€æœ¯è·¯çº¿å›¾)
- [ğŸ“„ è®¸å¯è¯](#ğŸ“„-è®¸å¯è¯)

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

### æ ¸å¿ƒä»·å€¼
æœ¬åº“è§£å†³äº†ä¼ ç»Ÿæ–‡æœ¬åˆ†å—æ–¹æ³•çš„æ ¹æœ¬é—®é¢˜ï¼š**å¦‚ä½•åœ¨ä¿æŒæ–‡æ¡£è¯­ä¹‰å®Œæ•´æ€§çš„å‰æä¸‹ï¼Œå®ç°æ™ºèƒ½åŒ–çš„æ–‡æœ¬åˆ‡åˆ†**ã€‚é€šè¿‡ASTè§£æå’Œç»“æ„æ„ŸçŸ¥ç®—æ³•ï¼Œç¡®ä¿æ¯ä¸ªæ–‡æœ¬å—éƒ½ä¿æŒè¯­ä¹‰è¾¹ç•Œçš„å®Œæ•´æ€§ã€‚

### æŠ€æœ¯ç‰¹è‰²
- ğŸ§  **ASTé©±åŠ¨**: åŸºäºæŠ½è±¡è¯­æ³•æ ‘çš„æ™ºèƒ½è§£æï¼Œè€Œéç®€å•çš„å­—ç¬¦åˆ†å‰²
- ğŸ“Š **ç»“æ„æ„ŸçŸ¥**: ç†è§£Markdownæ–‡æ¡£ç»“æ„ï¼Œä¿æŒæ ‡é¢˜ã€æ®µè½ã€è¡¨æ ¼çš„å®Œæ•´æ€§
- ğŸ¯ **è¯­ä¹‰è¾¹ç•Œ**: æ™ºèƒ½è¯†åˆ«è¯­ä¹‰è¾¹ç•Œï¼Œé¿å…åœ¨å¥å­ä¸­é—´åˆ‡æ–­
- âš¡ **é«˜æ€§èƒ½**: ç”Ÿäº§çº§æ€§èƒ½ä¼˜åŒ–ï¼Œæ”¯æŒå¤§è§„æ¨¡æ–‡æ¡£å¤„ç†
- ğŸ›¡ï¸ **ä¼ä¸šçº§**: å®Œæ•´çš„é”™è¯¯å¤„ç†ã€ç›‘æ§å’Œé…ç½®ç®¡ç†

### ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”

| ç‰¹æ€§ | ä¼ ç»Ÿå­—ç¬¦åˆ†å‰² | æ»‘åŠ¨çª—å£åˆ†å‰² | **ASTæ™ºèƒ½åˆ†å—** |
|------|-------------|-------------|----------------|
| è¯­ä¹‰å®Œæ•´æ€§ | âŒ ç»å¸¸åˆ‡æ–­å¥å­ | âš ï¸ éƒ¨åˆ†ä¿æŒ | âœ… å®Œå…¨ä¿æŒ |
| ç»“æ„æ„ŸçŸ¥ | âŒ æ— ç»“æ„ç†è§£ | âŒ æ— ç»“æ„ç†è§£ | âœ… æ·±åº¦ç†è§£ |
| è¡¨æ ¼å¤„ç† | âŒ ç ´åè¡¨æ ¼ | âŒ ç ´åè¡¨æ ¼ | âœ… æ™ºèƒ½å¤„ç† |
| æ ‡é¢˜å±‚æ¬¡ | âŒ ä¸¢å¤±ä¸Šä¸‹æ–‡ | âš ï¸ éƒ¨åˆ†ä¿æŒ | âœ… å®Œæ•´ç»´æŠ¤ |
| æ€§èƒ½ | âœ… å¾ˆå¿« | âœ… è¾ƒå¿« | âœ… é«˜æ•ˆ |
| å†…å­˜ä½¿ç”¨ | âœ… å¾ˆä½ | âš ï¸ ä¸­ç­‰ | âœ… ä¼˜åŒ– |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

#### æ–¹å¼1: ç›´æ¥ä½¿ç”¨ï¼ˆæ¨èï¼‰
```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/Lauorie/chunking.git
cd chunking

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### æ–¹å¼2: ä½œä¸ºPythonåŒ…å®‰è£…
```bash
# å¼€å‘ä¸­ï¼šå³å°†æ”¯æŒpipå®‰è£…
pip install ast-chunking
```

#### ä¾èµ–è¦æ±‚

**å¿…éœ€ä¾èµ–:**
```
Python >= 3.10
mistletoe >= 1.0.0
pydantic >= 2.0.0
tiktoken >= 0.5.0  # å¯é€‰ï¼Œç”¨äºOpenAI tokenizer
```

**å¯é€‰ä¾èµ–:**
```
transformers >= 4.20.0  # ç”¨äºHugging Faceæ¨¡å‹
loguru >= 0.6.0        # å¢å¼ºæ—¥å¿—åŠŸèƒ½
psutil >= 5.8.0        # æ€§èƒ½ç›‘æ§
```

#### éªŒè¯å®‰è£…
```python
from chunking import AstMarkdownSplitter
print("âœ… å®‰è£…æˆåŠŸï¼")

# å¿«é€Ÿæµ‹è¯•
splitter = AstMarkdownSplitter()
chunks = splitter.split_text("# æµ‹è¯•\nè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ®µè½ã€‚")
print(f"ç”Ÿæˆ {len(chunks)} ä¸ªå—")
```

### 5åˆ†é’Ÿä¸Šæ‰‹

```python
from chunking import AstMarkdownSplitter, setup_logging

# 1. è®¾ç½®æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
setup_logging(level="INFO")

# 2. åˆ›å»ºåˆ†å—å™¨
splitter = AstMarkdownSplitter(
    chunk_size=1024,     # æ¯å—æœ€å¤§1024ä¸ªtoken
    chunk_overlap=20     # å—é—´é‡å 20ä¸ªtoken
)

# 3. å‡†å¤‡æ–‡æ¡£
document = """
# äººå·¥æ™ºèƒ½ç®€ä»‹

## ä»€ä¹ˆæ˜¯AIï¼Ÿ
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯æŒ‡è®©æœºå™¨å…·å¤‡ç±»ä¼¼äººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚

## ä¸»è¦åº”ç”¨
- è‡ªç„¶è¯­è¨€å¤„ç†
- è®¡ç®—æœºè§†è§‰  
- æœºå™¨å­¦ä¹ 

## å‘å±•è¶‹åŠ¿
AIæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæœªæ¥å°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥ä½œç”¨ã€‚
"""

# 4. æ‰§è¡Œåˆ†å—
chunks = splitter.split_text(document)

# 5. æŸ¥çœ‹ç»“æœ
print(f"ğŸ“„ æ–‡æ¡£è¢«åˆ†å‰²ä¸º {len(chunks)} ä¸ªè¯­ä¹‰å—")
for i, chunk in enumerate(chunks, 1):
    print(f"\nğŸ§© å— {i}:")
    print("â”€" * 50)
    print(chunk)
    print("â”€" * 50)
    print(f"ğŸ“ é•¿åº¦: {len(chunk)} å­—ç¬¦")
```

**é¢„æœŸè¾“å‡º:**
```
ğŸ“„ æ–‡æ¡£è¢«åˆ†å‰²ä¸º 2 ä¸ªè¯­ä¹‰å—

ğŸ§© å— 1:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# äººå·¥æ™ºèƒ½ç®€ä»‹

## ä»€ä¹ˆæ˜¯AIï¼Ÿ
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯æŒ‡è®©æœºå™¨å…·å¤‡ç±»ä¼¼äººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚

## ä¸»è¦åº”ç”¨
- è‡ªç„¶è¯­è¨€å¤„ç†
- è®¡ç®—æœºè§†è§‰  
- æœºå™¨å­¦ä¹ 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ é•¿åº¦: 128 å­—ç¬¦

ğŸ§© å— 2:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# äººå·¥æ™ºèƒ½ç®€ä»‹

## å‘å±•è¶‹åŠ¿
AIæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæœªæ¥å°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥ä½œç”¨ã€‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ é•¿åº¦: 67 å­—ç¬¦
```

### æ ¸å¿ƒæ¦‚å¿µ

#### 1. ğŸŒ³ ASTï¼ˆæŠ½è±¡è¯­æ³•æ ‘ï¼‰
ASTæ˜¯æ–‡æ¡£ç»“æ„çš„æ ‘å½¢è¡¨ç¤ºï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªè¯­æ³•å…ƒç´ ï¼ˆæ ‡é¢˜ã€æ®µè½ã€è¡¨æ ¼ç­‰ï¼‰ã€‚

```
Document
â”œâ”€â”€ Heading(level=1) "äººå·¥æ™ºèƒ½ç®€ä»‹"
â”œâ”€â”€ Heading(level=2) "ä»€ä¹ˆæ˜¯AIï¼Ÿ"
â”œâ”€â”€ Paragraph "äººå·¥æ™ºèƒ½æ˜¯æŒ‡..."
â”œâ”€â”€ Heading(level=2) "ä¸»è¦åº”ç”¨" 
â”œâ”€â”€ List
â”‚   â”œâ”€â”€ ListItem "è‡ªç„¶è¯­è¨€å¤„ç†"
â”‚   â”œâ”€â”€ ListItem "è®¡ç®—æœºè§†è§‰"
â”‚   â””â”€â”€ ListItem "æœºå™¨å­¦ä¹ "
â””â”€â”€ Heading(level=2) "å‘å±•è¶‹åŠ¿"
```

#### 2. ğŸ¯ è¯­ä¹‰è¾¹ç•Œ
è¯­ä¹‰è¾¹ç•Œæ˜¯æŒ‡å†…å®¹åœ¨é€»è¾‘ä¸Šçš„è‡ªç„¶åˆ†å‰²ç‚¹ï¼Œå¦‚ï¼š
- å¥å­ç»“æŸï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰
- æ®µè½ç»“æŸï¼ˆåŒæ¢è¡Œï¼‰
- ç« èŠ‚ç»“æŸï¼ˆæ ‡é¢˜å˜åŒ–ï¼‰
- ç»“æ„ç»“æŸï¼ˆè¡¨æ ¼ã€åˆ—è¡¨ç»“æŸï¼‰

#### 3. ğŸ“Š Tokenè®¡ç®—
Tokenæ˜¯æ–‡æœ¬çš„æœ€å°å¤„ç†å•å…ƒï¼Œå¯ä»¥æ˜¯å­—ç¬¦ã€å•è¯æˆ–å­è¯ã€‚æœ¬åº“æ”¯æŒå¤šç§tokenizerï¼š

```python
# å­—ç¬¦çº§tokenizerï¼ˆé»˜è®¤ï¼‰
default_tokenizer = lambda text: text  # æ¯ä¸ªå­—ç¬¦1ä¸ªtoken

# OpenAI tokenizer
import tiktoken
openai_tokenizer = tiktoken.encoding_for_model("gpt-4").encode

# Hugging Face tokenizer
from transformers import AutoTokenizer
hf_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B").encode

# ä½¿ç”¨è‡ªå®šä¹‰tokenizer
splitter = AstMarkdownSplitter(tokenizer=openai_tokenizer)
```

#### 4. ğŸ·ï¸ å…ƒæ•°æ®æ„ŸçŸ¥
å…ƒæ•°æ®æ˜¯æè¿°æ–‡æ¡£çš„é™„åŠ ä¿¡æ¯ï¼Œå¦‚ä½œè€…ã€åˆ›å»ºæ—¶é—´ã€åˆ†ç±»ç­‰ã€‚å…ƒæ•°æ®æ„ŸçŸ¥åˆ†å—ä¼šä¸ºæ¯ä¸ªå—é¢„ç•™å…ƒæ•°æ®ç©ºé—´ï¼š

```python
metadata = "ä½œè€…: å¼ ä¸‰ | æ—¶é—´: 2024-01-15 | åˆ†ç±»: æŠ€æœ¯æ–‡æ¡£"
chunks = splitter.split_text_metadata_aware(document, metadata)
# æ¯ä¸ªchunkéƒ½ä¼šè€ƒè™‘å…ƒæ•°æ®å ç”¨çš„tokenç©ºé—´
```

#### 5. ğŸ“ æœ€å°å—å¤§å°æ§åˆ¶ (min_chunk_tokens)
`min_chunk_tokens` å‚æ•°æ§åˆ¶åˆ†å—çš„æœ€å°å¤§å°ï¼Œç¡®ä¿æ¯ä¸ªå—éƒ½æœ‰è¶³å¤Ÿçš„å†…å®¹æ¥ä¿è¯è¯­ä¹‰å®Œæ•´æ€§å’Œæ£€ç´¢è´¨é‡ï¼š

```python
# é…ç½®æœ€å°å—å¤§å°
splitter = AstMarkdownSplitter(
    chunk_size=1024,
    min_chunk_tokens=50  # å°äº50ä¸ªtokençš„å—ä¼šè¢«åˆå¹¶
)

# è‡ªåŠ¨åˆå¹¶è¿‡å°çš„å—
def merge_small_chunks_example():
    """æ¼”ç¤ºå°å—åˆå¹¶æœºåˆ¶"""
    # åŸå§‹åˆ†å—ï¼š["çŸ­æ ‡é¢˜", "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„å†…å®¹..."]
    # å¦‚æœ"çŸ­æ ‡é¢˜"åªæœ‰15ä¸ªtokenï¼ˆ< 50ï¼‰ï¼Œä¼šè¢«åˆå¹¶ä¸ºï¼š
    # ["çŸ­æ ‡é¢˜\n\nè¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„å†…å®¹..."]
    pass
```

**ä¸ºä»€ä¹ˆéœ€è¦æœ€å°å—å¤§å°æ§åˆ¶ï¼Ÿ**

1. **æé«˜æ£€ç´¢è´¨é‡**: è¿‡å°çš„å—ï¼ˆå¦‚å•ç‹¬çš„æ ‡é¢˜ï¼‰ç¼ºä¹ä¸Šä¸‹æ–‡ï¼Œå½±å“å‘é‡æ£€ç´¢æ•ˆæœ
2. **ä¼˜åŒ–å­˜å‚¨æ•ˆç‡**: å‡å°‘ç¢ç‰‡åŒ–çš„å°å—ï¼Œé™ä½å­˜å‚¨å’Œç´¢å¼•å¼€é”€  
3. **å¢å¼ºè¯­ä¹‰å®Œæ•´æ€§**: ç¡®ä¿æ¯ä¸ªå—éƒ½åŒ…å«è¶³å¤Ÿçš„è¯­ä¹‰ä¿¡æ¯
4. **æ”¹å–„ç”¨æˆ·ä½“éªŒ**: é¿å…æ£€ç´¢åˆ°æ²¡æœ‰å®é™…å†…å®¹çš„æ ‡é¢˜å—

**åˆå¹¶ç­–ç•¥ï¼š**
- **æ™ºèƒ½æ ‡é¢˜åˆå¹¶**: å°æ ‡é¢˜ä¼šä¸åç»­å†…å®¹åˆå¹¶
- **æœ«å°¾å—å¤„ç†**: æ–‡æ¡£æœ«å°¾çš„å°å—ä¼šä¸å‰ä¸€å—åˆå¹¶ï¼ˆå¦‚æœä¸è¶…è¿‡chunk_sizeï¼‰
- **ä¿æŒç»“æ„**: åˆå¹¶è¿‡ç¨‹ä¿æŒåŸæœ‰çš„å±‚æ¬¡ç»“æ„å’Œæ ¼å¼

```python
# å®é™…åˆå¹¶ç¤ºä¾‹
original_chunks = [
    "# å‚è€ƒæ–‡æ¡£",           # 15 tokens (< 50)
    "è¯¦ç»†çš„å‚è€ƒä¿¡æ¯...",      # 200 tokens  
    "## å°èŠ‚æ ‡é¢˜",           # 12 tokens (< 50)
    "å°èŠ‚å†…å®¹å¾ˆå°‘",          # 18 tokens (< 50)
    "å¤§é‡çš„æ–‡æ¡£å†…å®¹..."       # 300 tokens
]

merged_chunks = [
    "# å‚è€ƒæ–‡æ¡£\n\nè¯¦ç»†çš„å‚è€ƒä¿¡æ¯...",  # 215 tokens
    "## å°èŠ‚æ ‡é¢˜\n\nå°èŠ‚å†…å®¹å¾ˆå°‘\n\nå¤§é‡çš„æ–‡æ¡£å†…å®¹..."  # 330 tokens
]
```

## ğŸ”¬ ç®—æ³•åŸç†

### 1. ASTè§£ææœºåˆ¶

#### 1.1 Markdown ASTæ„å»º
```python
# è¾“å…¥Markdownæ–‡æœ¬
markdown_text = """
# ä¸»æ ‡é¢˜
## å‰¯æ ‡é¢˜
æ®µè½å†…å®¹...
| è¡¨æ ¼ | åˆ— |
|------|-----|
| æ•°æ® | å€¼ |
"""

# ASTè§£æè¿‡ç¨‹
with mistletoe.markdown_renderer.MarkdownRenderer() as renderer:
    doc = mistletoe.Document(markdown_text)
    # ç”ŸæˆASTæ ‘ç»“æ„:
    # Document
    # â”œâ”€â”€ Heading(level=1) "ä¸»æ ‡é¢˜"
    # â”œâ”€â”€ Heading(level=2) "å‰¯æ ‡é¢˜" 
    # â”œâ”€â”€ Paragraph "æ®µè½å†…å®¹..."
    # â””â”€â”€ Table
    #     â”œâ”€â”€ TableRow(header)
    #     â””â”€â”€ TableRow(data)
```

#### 1.2 ASTéå†ç®—æ³•
```python
def _split_document(self, doc: Document, max_chunk_size: int) -> List[str]:
    """
    æ ¸å¿ƒåˆ†å—ç®—æ³• - æ·±åº¦ä¼˜å…ˆéå†ASTèŠ‚ç‚¹
    
    ç®—æ³•æµç¨‹:
    1. ç»´æŠ¤å½“å‰å—å†…å®¹å’Œå¤§å°
    2. éå†ASTèŠ‚ç‚¹ï¼Œè®¡ç®—èŠ‚ç‚¹å¤§å°
    3. æ ¹æ®å¤§å°å†³å®šæ˜¯å¦æ”¾å…¥å½“å‰å—æˆ–å¼€å§‹æ–°å—
    4. ç‰¹æ®Šå¤„ç†æ ‡é¢˜å±‚æ¬¡ç»“æ„
    5. é€’å½’åˆ†å‰²è¿‡å¤§çš„èŠ‚ç‚¹
    """
    chunks = []
    headers = {}  # ç»´æŠ¤æ ‡é¢˜å±‚æ¬¡ç»“æ„
    total_size = 0
    block_contents = []
    
    while doc.children:
        child = doc.children.pop(0)
        block_content, block_size = self._render_and_tokenize(child)
        
        # æ ¸å¿ƒå†³ç­–é€»è¾‘
        if self._can_fit_in_current_chunk(total_size, block_size, max_chunk_size):
            # æ”¾å…¥å½“å‰å—
            block_contents.append(block_content)
            total_size += block_size
        elif self._can_fit_in_new_chunk(block_size, headers, max_chunk_size):
            # å¼€å§‹æ–°å—ï¼Œä¿æŒæ ‡é¢˜å±‚æ¬¡
            self._flush_current_chunk(chunks, block_contents)
            self._start_new_chunk_with_headers(doc, child, headers)
        else:
            # èŠ‚ç‚¹è¿‡å¤§ï¼Œéœ€è¦é€’å½’åˆ†å‰²
            split_nodes = self._split_oversized_node(child, max_chunk_size)
            doc.children = split_nodes + doc.children
```

### 2. æ™ºèƒ½åˆ†å—ç­–ç•¥

#### 2.1 è¯­ä¹‰è¾¹ç•Œè¯†åˆ«
```python
def _split_paragraph(self, paragraph: Paragraph, max_size: int) -> List[BlockToken]:
    """
    æ®µè½æ™ºèƒ½åˆ†å‰²ç®—æ³•
    
    ç­–ç•¥:
    1. æŒ‰å¥å­åˆ†å‰²ï¼ˆè¯†åˆ«å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰ï¼‰
    2. è®¡ç®—æ¯ä¸ªå¥å­çš„tokenæ•°é‡
    3. è´ªå¿ƒç®—æ³•ç»„åˆå¥å­ï¼Œä¸è¶…è¿‡max_size
    4. ç¡®ä¿ä¸åœ¨å¥å­ä¸­é—´åˆ‡æ–­
    """
    text = self._render_block(paragraph)
    
    # å¥å­è¾¹ç•Œè¯†åˆ«
    sentences = self._split_by_sentence_boundaries(text)
    sentence_tokens = [len(self._tokenizer(sent)) for sent in sentences]
    
    # è´ªå¿ƒç»„åˆç®—æ³•
    groups = []
    current_group = []
    current_size = 0
    
    for sentence, token_count in zip(sentences, sentence_tokens):
        if current_size + token_count <= max_size:
            current_group.append(sentence)
            current_size += token_count
        else:
            if current_group:
                groups.append(''.join(current_group))
            current_group = [sentence]
            current_size = token_count
    
    return [Document(group).children[0] for group in groups if group]
```

#### 2.2 è¡¨æ ¼å¤„ç†ç®—æ³•
```python
def _split_table(self, table: Table, max_size: int) -> List[BlockToken]:
    """
    è¡¨æ ¼æ™ºèƒ½å¤„ç†ç®—æ³•
    
    ç­–ç•¥:
    1. åˆ†æè¡¨å¤´å¤§å°ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºæ•°æ®è¡¨å¤´
    2. è®¡ç®—æ¯è¡Œçš„tokenæ•°é‡
    3. æ ¹æ®convert_table_ratioå†³å®šæ˜¯å¦è½¬æ¢ä¸ºæ®µè½
    4. æŒ‰è¡Œåˆ†å‰²ï¼Œä¿æŒè¡¨æ ¼ç»“æ„å®Œæ•´æ€§
    """
    header_size = self._calculate_row_size(table.header)
    row_sizes = [self._calculate_row_size(row) for row in table.children]
    
    # æ™ºèƒ½è¡¨å¤´åˆ¤æ–­
    avg_row_size = sum(row_sizes) / len(row_sizes) if row_sizes else 0
    if header_size > avg_row_size * self.header_threshold:
        # è¡¨å¤´è¿‡å¤§ï¼Œå¯èƒ½æ˜¯æè¿°æ€§æ–‡æœ¬ï¼Œè½¬ä¸ºæ™®é€šè¡Œ
        table = self._normalize_table_header(table)
    
    # è¡¨æ ¼è½¬æ®µè½åˆ¤æ–­
    max_row_size = max(row_sizes) if row_sizes else 0
    if max_row_size >= max_size * self.convert_table_ratio:
        return self._convert_table_to_paragraphs(table)
    
    # æŒ‰è¡Œåˆ†å‰²è¡¨æ ¼
    return self._split_table_by_rows(table, max_size)
```


##### convert_table_ratio çš„ä½œç”¨

`convert_table_ratio` å‚æ•°æ˜¯ç”¨äºæ§åˆ¶è¡¨æ ¼å¤„ç†æ–¹å¼çš„é˜ˆå€¼å‚æ•°ã€‚è¿™ä¸ªå‚æ•°å†³å®šäº†ä½•æ—¶å°†è¡¨æ ¼ä»åŸå§‹çš„è¡¨æ ¼æ ¼å¼è½¬æ¢ä¸ºæ®µè½æ ¼å¼ã€‚è¯¦ç»†è§£é‡Šå¦‚ä¸‹ï¼š

åœ¨ä»£ç ä¸­ï¼Œ`convert_table_ratio` çš„é»˜è®¤å€¼è®¾ç½®ä¸º 0.5ï¼š

```python
convert_table_ratio: float = Field(
    default=0.5,
    description="The ratio of the max_chunk_size to convert table to paragraph.",
    gt=0,
)
```

è¿™ä¸ªå‚æ•°åœ¨ `_split_table` æ–¹æ³•ä¸­è¢«ä½¿ç”¨ï¼š

```python
# convert to paragraph block
if max(table_row_sizes) >= self.chunk_size * self.convert_table_ratio:
    return self._convert_table_to_paragraph(table)
```

##### å·¥ä½œåŸç†

1. å½“ç®—æ³•å¤„ç†è¡¨æ ¼æ—¶ï¼Œé¦–å…ˆè®¡ç®—è¡¨æ ¼æ¯ä¸€è¡Œçš„tokenå¤§å° (`table_row_sizes`)
2. å¦‚æœè¡¨æ ¼ä¸­ä»»ä½•ä¸€è¡Œçš„tokenæ•°é‡è¶…è¿‡äº† `chunk_size * convert_table_ratio`ï¼Œåˆ™æ•´ä¸ªè¡¨æ ¼ä¼šè¢«è½¬æ¢ä¸ºæ®µè½æ ¼å¼
3. è¿™æ„å‘³ç€ï¼Œå¦‚æœ `convert_table_ratio = 0.5` ä¸” `chunk_size = 1000`ï¼Œé‚£ä¹ˆå½“è¡¨æ ¼ä¸­æœ‰ä»»ä½•ä¸€è¡Œè¶…è¿‡500ä¸ªtokenæ—¶ï¼Œæ•´ä¸ªè¡¨æ ¼ä¼šè¢«è½¬æ¢ä¸ºæ®µè½

##### ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå‚æ•°

è¡¨æ ¼æ˜¯ä¸€ç§ç‰¹æ®Šçš„ç»“æ„åŒ–æ•°æ®ï¼Œå½“å®ƒå¾ˆå¤§æ—¶ï¼Œç®€å•åœ°åˆ‡åˆ†å¯èƒ½ä¼šç ´åå…¶ç»“æ„å’Œå¯è¯»æ€§ã€‚å°†å…¶è½¬æ¢ä¸ºæ®µè½æ ¼å¼æœ‰å‡ ä¸ªå¥½å¤„ï¼š

1. **é¿å…è¡¨æ ¼ç»“æ„è¢«ç ´å**ï¼šå¦‚æœè¡¨æ ¼å¤ªå¤§è€Œå¿…é¡»è·¨å¤šä¸ªå—ï¼ŒåŸå§‹çš„è¡¨æ ¼æ ¼å¼å¯èƒ½ä¼šå˜å¾—æ··ä¹±æˆ–ä¸å¯è¯»
2. **æ›´å¥½çš„åˆ†å—æ•ˆç‡**ï¼šæ®µè½æ ¼å¼æ›´å®¹æ˜“åˆ†å‰²æˆå°å—
3. **æé«˜æ ‡è®°åŒ–æ•ˆç‡**ï¼šæŸäº›æ–‡æœ¬å¤„ç†ç³»ç»Ÿå¯¹è¡¨æ ¼çš„å¤„ç†ä¸å¦‚å¯¹æ™®é€šæ–‡æœ¬é«˜æ•ˆ

##### è½¬æ¢åçš„æ ¼å¼

å½“è¡¨æ ¼è¢«è½¬æ¢ä¸ºæ®µè½æ—¶ï¼Œ`_convert_table_to_paragraph` æ–¹æ³•å°†è¡¨æ ¼çš„æ¯ä¸€è¡Œè½¬æ¢ä¸ºé”®å€¼å¯¹æ ¼å¼çš„æ®µè½ï¼š

```
è¡¨å¤´1: å•å…ƒæ ¼1å†…å®¹    è¡¨å¤´2: å•å…ƒæ ¼2å†…å®¹    è¡¨å¤´3: å•å…ƒæ ¼3å†…å®¹
```

è¿™ç§æ ¼å¼ä¿ç•™äº†è¡¨æ ¼çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä½†ä»¥çº¿æ€§æ–¹å¼å‘ˆç°ï¼Œæ›´é€‚åˆäºè¿›ä¸€æ­¥çš„åˆ†å—å¤„ç†ã€‚

##### è°ƒæ•´å»ºè®®

- **å¢å¤§ `convert_table_ratio`**ï¼ˆå¦‚0.8æˆ–0.9ï¼‰ï¼šæ›´å¤šè¡¨æ ¼ä¼šä¿æŒåŸå§‹è¡¨æ ¼æ ¼å¼ï¼Œåªæœ‰éå¸¸å¤§çš„è¡¨æ ¼æ‰ä¼šè½¬æ¢ä¸ºæ®µè½
- **å‡å° `convert_table_ratio`**ï¼ˆå¦‚0.3æˆ–0.2ï¼‰ï¼šæ›´å¤šè¡¨æ ¼ä¼šè¢«è½¬æ¢ä¸ºæ®µè½æ ¼å¼ï¼Œå³ä½¿å®ƒä»¬ç›¸å¯¹è¾ƒå°

é€‰æ‹©åˆé€‚çš„å€¼å–å†³äºä½ çš„å…·ä½“éœ€æ±‚å’Œå¤„ç†çš„è¡¨æ ¼ç±»å‹ã€‚å¦‚æœä½ çš„æ–‡æ¡£ä¸­æœ‰å¾ˆå¤šå¤æ‚çš„è¡¨æ ¼ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¿™ä¸ªå‚æ•°ä»¥è·å¾—æœ€ä½³çš„åˆ†å—æ•ˆæœã€‚

---



### 3. å±‚æ¬¡ç»“æ„ç»´æŠ¤

#### 3.1 æ ‡é¢˜ä¸Šä¸‹æ–‡ç®—æ³•
```python
def _maintain_header_context(self, headers: Dict, current_chunk: List, new_child: BlockToken):
    """
    æ ‡é¢˜å±‚æ¬¡ç»“æ„ç»´æŠ¤ç®—æ³•
    
    ç›®æ ‡: ç¡®ä¿æ¯ä¸ªå—éƒ½åŒ…å«å®Œæ•´çš„æ ‡é¢˜ä¸Šä¸‹æ–‡
    
    ç®—æ³•:
    1. ç»´æŠ¤å½“å‰æ´»è·ƒçš„æ ‡é¢˜å±‚æ¬¡
    2. æ–°å—å¼€å§‹æ—¶ï¼Œå¤åˆ¶ç›¸å…³æ ‡é¢˜
    3. é‡åˆ°æ–°æ ‡é¢˜æ—¶ï¼Œæ›´æ–°å±‚æ¬¡ç»“æ„
    4. æ¸…ç†è¿‡æ—¶çš„ä¸‹çº§æ ‡é¢˜
    """
    if isinstance(new_child, Heading):
        # æ›´æ–°æ ‡é¢˜å±‚æ¬¡
        headers[new_child.level] = (new_child, self._calculate_size(new_child))
        
        # æ¸…ç†ä¸‹çº§æ ‡é¢˜
        for level in list(headers.keys()):
            if level > new_child.level:
                del headers[level]
    
    # è®¡ç®—æ·»åŠ æ ‡é¢˜ä¸Šä¸‹æ–‡çš„æˆæœ¬
    header_cost = sum(size for _, size in headers.values())
    return header_cost


""" å—åŒ…å«å®Œæ•´çš„æ ‡é¢˜ä¸Šä¸‹æ–‡ä¾‹å­:
# 4 è¿æ¥æ€ç§»åŠ¨æ€§åŸºç¡€åŠŸèƒ½\n\n## 4.1 åŸç†æè¿°\n\n### 4.1.4 æµ‹é‡æ§åˆ¶ä¸‹å‘ æ­£æ–‡éƒ¨åˆ†...
"""
```

#### 3.2 å—é‡ç»„ç®—æ³•
```python
def _reorganize_chunks_with_context(self, raw_chunks: List, headers: Dict):
    """
    å—é‡ç»„ç®—æ³• - æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    ç­–ç•¥:
    1. ä¸ºæ¯ä¸ªå—æ·»åŠ å¿…è¦çš„æ ‡é¢˜ä¸Šä¸‹æ–‡
    2. ç¡®ä¿è¯­ä¹‰è¿è´¯æ€§
    3. ä¼˜åŒ–å—ä¹‹é—´çš„é‡å 
    """
    enhanced_chunks = []
    
    for chunk in raw_chunks:
        # åˆ†æå—çš„å†…å®¹ç±»å‹
        content_analysis = self._analyze_chunk_content(chunk)
        
        # ç¡®å®šéœ€è¦çš„æ ‡é¢˜ä¸Šä¸‹æ–‡
        required_headers = self._determine_required_headers(content_analysis, headers)
        
        # é‡ç»„å—å†…å®¹
        enhanced_chunk = self._reconstruct_chunk_with_context(
            chunk, required_headers
        )
        enhanced_chunks.append(enhanced_chunk)
    
    return enhanced_chunks
```

### 4. å…ƒæ•°æ®æ„ŸçŸ¥åˆ†å—

#### 4.1 å…ƒæ•°æ®ç©ºé—´è®¡ç®—
```python
def split_text_metadata_aware(self, text: str, metadata: str) -> List[str]:
    """
    å…ƒæ•°æ®æ„ŸçŸ¥åˆ†å—ç®—æ³•
    
    æ ¸å¿ƒæ€æƒ³: ä¸ºæ¯ä¸ªå—é¢„ç•™å…ƒæ•°æ®ç©ºé—´ï¼Œç¡®ä¿æœ€ç»ˆå—ä¸è¶…è¿‡é™åˆ¶
    
    è®¡ç®—å…¬å¼:
    effective_chunk_size = configured_chunk_size - metadata_token_count - format_overhead
    """
    metadata_tokens = len(self._tokenizer(metadata))
    format_overhead = self.CONFIG.default_metadata_format_len  # JSONç­‰æ ¼å¼å¼€é”€
    
    effective_size = self.chunk_size - metadata_tokens - format_overhead
    
    if effective_size <= 0:
        raise ValueError("å…ƒæ•°æ®è¿‡é•¿ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†å—")
    
    return self._split_text(text, effective_size)
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### 1. æ¨¡å—æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               chunking/                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  __init__.py    - åŒ…æ¥å£å’ŒAPIå¯¼å‡º        â”‚
â”‚  chunking.py    - æ ¸å¿ƒåˆ†å—ç®—æ³•å®ç°        â”‚
â”‚  config.py      - é…ç½®ç®¡ç†å’Œç¯å¢ƒå˜é‡      â”‚
â”‚  utils.py       - å·¥å…·å‡½æ•°å’Œæ€§èƒ½ç›‘æ§      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. ç±»å±‚æ¬¡ç»“æ„

```python
# æ ¸å¿ƒç±»è®¾è®¡
class MetadataAwareTextSplitter(BaseModel):
    """æŠ½è±¡åŸºç±» - å®šä¹‰åˆ†å—æ¥å£"""
    
class AstMarkdownSplitter(MetadataAwareTextSplitter):
    """å…·ä½“å®ç° - AST basedåˆ†å—"""
    
    # æ ¸å¿ƒç®—æ³•æ–¹æ³•
    def _split_document(self, doc: Document, max_size: int) -> List[str]
    def _split_block(self, block: BlockToken, max_size: int) -> List[BlockToken]
    def _split_paragraph(self, para: Paragraph, max_size: int) -> List[BlockToken]
    def _split_table(self, table: Table, max_size: int) -> List[BlockToken]
    def _split_list(self, list_block: ListBlock, max_size: int) -> List[BlockToken]
```

### 3. æ•°æ®æµå›¾

```
è¾“å…¥æ–‡æœ¬
    â†“
[ASTè§£æ] â†’ Document Tree
    â†“
[èŠ‚ç‚¹éå†] â†’ é€ä¸ªå¤„ç†BlockToken
    â†“
[å¤§å°è®¡ç®—] â†’ åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å‰²
    â†“              â†“
[ç›´æ¥æ·»åŠ ]    [é€’å½’åˆ†å‰²]
    â†“              â†“
[å—ç»„è£…] â† â† â† â† â† â†
    â†“
[ä¸Šä¸‹æ–‡å¢å¼º] â†’ æ·»åŠ æ ‡é¢˜å±‚æ¬¡
    â†“
[æœ€ç»ˆè¾“å‡º] â†’ List[str]
```

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### 1. Tokenè®¡ç®—ä¼˜åŒ–

```python
# ç¼“å­˜æœºåˆ¶
@lru_cache(maxsize=1000)
def _cached_tokenize(self, text: str) -> int:
    """ç¼“å­˜åˆ†è¯ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—"""
    return len(self._tokenizer(text))

# å¢é‡è®¡ç®—
def _incremental_size_calculation(self, current_size: int, new_block: str) -> int:
    """å¢é‡è®¡ç®—ï¼Œé¿å…é‡å¤tokenization"""
    new_block_size = self._cached_tokenize(new_block)
    return current_size + new_block_size + 2  # +2 for separator
```

### 2. å†…å­˜ç®¡ç†ä¼˜åŒ–

```python
def _memory_efficient_processing(self, doc: Document):
    """å†…å­˜é«˜æ•ˆçš„å¤„ç†ç­–ç•¥"""
    # æµå¼å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å†…å®¹
    while doc.children:
        # å¤„ç†å•ä¸ªèŠ‚ç‚¹åç«‹å³é‡Šæ”¾
        child = doc.children.pop(0)
        yield self._process_node(child)
        del child  # æ˜¾å¼é‡Šæ”¾å†…å­˜
```

### 3. å¹¶å‘å¤„ç†æ”¯æŒ

```python
from concurrent.futures import ThreadPoolExecutor

def batch_split_texts(self, texts: List[str]) -> List[List[str]]:
    """æ‰¹é‡å¤„ç†æ”¯æŒå¹¶å‘"""
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(self.split_text, text) for text in texts]
        return [future.result() for future in futures]
```

## ğŸ”§ é«˜çº§é…ç½®

### 1. è‡ªå®šä¹‰åˆ†è¯å™¨é›†æˆ

```python
# é›†æˆHugging Face tokenizer
from transformers import AutoTokenizer

def create_hf_tokenizer(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return lambda text: tokenizer.encode(text, add_special_tokens=False)

# ä½¿ç”¨ç¤ºä¾‹
splitter = AstMarkdownSplitter(
    chunk_size=512,
    tokenizer=create_hf_tokenizer("Qwen3-8B")
)

# é›†æˆOpenAI tiktoken
import tiktoken

def create_tiktoken_tokenizer(encoding_name: str = "gpt-4o"):
    encoding = tiktoken.encoding_for_model(encoding_name)
    return lambda text: encoding.encode(text)

splitter = AstMarkdownSplitter(
    tokenizer=create_tiktoken_tokenizer("gpt-4o")
)
```

### 2. é«˜çº§åˆ†å—ç­–ç•¥

```python
# è‡ªå®šä¹‰è¡¨æ ¼å¤„ç†ç­–ç•¥
class CustomTableProcessor:
    def __init__(self, preserve_structure: bool = True):
        self.preserve_structure = preserve_structure
    
    def process_table(self, table: Table) -> str:
        if self.preserve_structure:
            return self._preserve_table_format(table)
        else:
            return self._convert_to_text(table)

# é›†æˆè‡ªå®šä¹‰å¤„ç†å™¨
splitter = AstMarkdownSplitter(
    table_processor=CustomTableProcessor(preserve_structure=False)
)
```

### 3. åŠ¨æ€é…ç½®è°ƒæ•´

```python
# è¿è¡Œæ—¶é…ç½®è°ƒæ•´
def adaptive_chunk_size(text_complexity: float, base_size: int = 1024) -> int:
    """æ ¹æ®æ–‡æœ¬å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å—å¤§å°"""
    if text_complexity > 0.8:  # å¤æ‚æ–‡æœ¬
        return int(base_size * 0.7)  # å‡å°å—å¤§å°
    elif text_complexity < 0.3:  # ç®€å•æ–‡æœ¬
        return int(base_size * 1.3)  # å¢å¤§å—å¤§å°
    return base_size

# æ–‡æœ¬å¤æ‚åº¦åˆ†æ
def analyze_text_complexity(text: str) -> float:
    """åˆ†ææ–‡æœ¬å¤æ‚åº¦ (0-1)"""
    factors = {
        'avg_sentence_length': len(text.split()) / len(text.split('.')),
        'table_density': text.count('|') / len(text),
        'heading_density': text.count('#') / len(text),
        'list_density': text.count('- ') / len(text)
    }
    return min(1.0, sum(factors.values()) / len(factors))
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. å¤„ç†é€Ÿåº¦åŸºå‡†

```python
# æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
import time
from chunking import AstMarkdownSplitter, metrics

def benchmark_processing_speed():
    splitter = AstMarkdownSplitter(chunk_size=1024)
    test_text = "# æ ‡é¢˜\n" + "è¿™æ˜¯æµ‹è¯•å†…å®¹ã€‚" * 1000
    
    start_time = time.time()
    chunks = splitter.split_text(test_text)
    end_time = time.time()
    
    processing_speed = len(test_text) / (end_time - start_time)
    print(f"å¤„ç†é€Ÿåº¦: {processing_speed:.0f} å­—ç¬¦/ç§’")
    print(f"ç”Ÿæˆå—æ•°: {len(chunks)}")
    
    # æŸ¥çœ‹è¯¦ç»†æŒ‡æ ‡
    summary = metrics.get_summary()
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {summary['avg_processing_time']:.3f}ç§’")
    print(f"ååé‡: {summary['throughput_chars_per_second']:.0f} å­—ç¬¦/ç§’")

# å…¸å‹æ€§èƒ½æŒ‡æ ‡
# - å¤„ç†é€Ÿåº¦: ~50,000 å­—ç¬¦/ç§’
# - å†…å­˜ä½¿ç”¨: ~2MB per 100KB æ–‡æœ¬
# - åˆ†å—å‡†ç¡®ç‡: >95% è¯­ä¹‰è¾¹ç•Œä¿æŒ
```

### 2. å†…å­˜ä½¿ç”¨åˆ†æ

```python
import psutil
import os

def memory_usage_analysis():
    process = psutil.Process(os.getpid())
    
    # å¤„ç†å‰å†…å­˜
    mem_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # å¤„ç†å¤§æ–‡æ¡£
    large_text = "# å¤§æ–‡æ¡£\n" + "å†…å®¹æ®µè½ã€‚" * 10000
    splitter = AstMarkdownSplitter()
    chunks = splitter.split_text(large_text)
    
    # å¤„ç†åå†…å­˜
    mem_after = process.memory_info().rss / 1024 / 1024  # MB
    
    print(f"å†…å­˜ä½¿ç”¨å¢é•¿: {mem_after - mem_before:.2f} MB")
    print(f"æ–‡æ¡£å¤§å°: {len(large_text) / 1024:.2f} KB")
    print(f"å†…å­˜æ•ˆç‡: {(len(large_text) / 1024) / (mem_after - mem_before):.2f} KBæ–‡æ¡£/MBå†…å­˜")
```

## ğŸ§ª å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€æ–‡æ¡£å¤„ç†

```python
from chunking import AstMarkdownSplitter, setup_logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
setup_logging(level="INFO")

# åˆ›å»ºåˆ†å—å™¨
splitter = AstMarkdownSplitter(
    chunk_size=1024,      # 1024 tokens per chunk
    chunk_overlap=20,     # 20 tokens overlap
    convert_table_ratio=0.5,  # è¡¨æ ¼è½¬æ¢é˜ˆå€¼
    enable_first_line_as_title=True  # é¦–æ®µè½¬æ ‡é¢˜
)

# ç¤ºä¾‹æ–‡æ¡£
document = """
# äººå·¥æ™ºèƒ½æŠ€æœ¯æŠ¥å‘Š

## æ‘˜è¦

äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚

## æŠ€æœ¯å‘å±•ç°çŠ¶

### æ·±åº¦å­¦ä¹ 

æ·±åº¦å­¦ä¹ æŠ€æœ¯åŸºäºç¥ç»ç½‘ç»œï¼Œé€šè¿‡å¤šå±‚æ¬¡çš„ç‰¹å¾å­¦ä¹ æ¥è§£å†³å¤æ‚é—®é¢˜ã€‚ä¸»è¦åŒ…æ‹¬ï¼š

1. **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰** - ä¸»è¦ç”¨äºå›¾åƒå¤„ç†
2. **å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰** - é€‚åˆåºåˆ—æ•°æ®å¤„ç†  
3. **å˜æ¢å™¨æ¨¡å‹ï¼ˆTransformerï¼‰** - ç°ä»£NLPçš„åŸºç¡€

### è‡ªç„¶è¯­è¨€å¤„ç†

| æŠ€æœ¯ | åº”ç”¨åœºæ™¯ | ä»£è¡¨æ¨¡å‹ |
|------|----------|----------|
| æ–‡æœ¬åˆ†ç±» | æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±» | BERT, RoBERTa |
| æ–‡æœ¬ç”Ÿæˆ | å¯¹è¯ç³»ç»Ÿã€å†…å®¹åˆ›ä½œ | GPT-3, ChatGPT |
| æœºå™¨ç¿»è¯‘ | è·¨è¯­è¨€äº¤æµ | T5, mBART |

## æœªæ¥å‘å±•è¶‹åŠ¿

äººå·¥æ™ºèƒ½æŠ€æœ¯å°†ç»§ç»­å‘ä»¥ä¸‹æ–¹å‘å‘å±•ï¼š

- æ›´å¼ºçš„é€šç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›
- æ›´é«˜çš„è®¡ç®—æ•ˆç‡å’Œèƒ½æºæ•ˆç‡
- æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦
- æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯è¦†ç›–

## ç»“è®º

äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä¸ºå„è¡Œå„ä¸šå¸¦æ¥äº†æ–°çš„æœºé‡å’ŒæŒ‘æˆ˜ï¼Œéœ€è¦æŒç»­å…³æ³¨æŠ€æœ¯è¿›å±•å¹¶åˆç†åº”ç”¨ã€‚
"""

# æ‰§è¡Œåˆ†å—
chunks = splitter.split_text(document)

print(f"æ–‡æ¡£è¢«åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
for i, chunk in enumerate(chunks, 1):
    print(f"\n=== å— {i} ===")
    print(chunk)
    print(f"é•¿åº¦: {len(chunk)} å­—ç¬¦")
```

### 2. å…ƒæ•°æ®æ„ŸçŸ¥å¤„ç†

```python
# å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£å¤„ç†
metadata = {
    "document_id": "AI_REPORT_2024_001",
    "author": "ç ”ç©¶å›¢é˜Ÿ",
    "department": "AIå®éªŒå®¤", 
    "created_date": "2024-01-15",
    "classification": "å†…éƒ¨æ–‡æ¡£",
    "version": "1.2"
}

# è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
metadata_str = " | ".join([f"{k}: {v}" for k, v in metadata.items()])

# å…ƒæ•°æ®æ„ŸçŸ¥åˆ†å—
chunks_with_metadata = splitter.split_text_metadata_aware(document, metadata_str)

print(f"å¸¦å…ƒæ•°æ®çš„åˆ†å—ç»“æœ: {len(chunks_with_metadata)} ä¸ªå—")
for i, chunk in enumerate(chunks_with_metadata, 1):
    print(f"\n=== å…ƒæ•°æ®å— {i} ===")
    print(f"å…ƒæ•°æ®: {metadata_str}")
    print(f"å†…å®¹: {chunk[:200]}...")
```

### 3. æ‰¹é‡æ–‡æ¡£å¤„ç†

```python
import os
from pathlib import Path
from chunking import timer, metrics, format_chunk_stats

def process_document_batch(document_dir: str, output_dir: str):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£ç›®å½•"""
    splitter = AstMarkdownSplitter(chunk_size=512, chunk_overlap=30)
    
    # é‡ç½®æ€§èƒ½æŒ‡æ ‡
    metrics.reset()
    
    doc_files = list(Path(document_dir).glob("*.md"))
    
    with timer(f"æ‰¹é‡å¤„ç† {len(doc_files)} ä¸ªæ–‡æ¡£"):
        for doc_file in doc_files:
            try:
                # è¯»å–æ–‡æ¡£
                with open(doc_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åˆ†å—å¤„ç†
                chunks = splitter.split_text(content)
                
                # ä¿å­˜ç»“æœ
                output_file = Path(output_dir) / f"{doc_file.stem}_chunks.json"
                save_chunks_to_json(chunks, output_file, doc_file.name)
                
                print(f"âœ“ {doc_file.name}: {format_chunk_stats(chunks)}")
                
            except Exception as e:
                print(f"âœ— {doc_file.name}: å¤„ç†å¤±è´¥ - {e}")
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    summary = metrics.get_summary()
    print(f"\næ‰¹é‡å¤„ç†å®Œæˆ:")
    print(f"  æ€»æ“ä½œæ•°: {summary['total_operations']}")
    print(f"  æ€»å—æ•°: {summary['total_chunks_generated']}")
    print(f"  æ€»å­—ç¬¦æ•°: {summary['total_chars_processed']}")
    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {summary['avg_processing_time']:.3f}ç§’")
    print(f"  ååé‡: {summary['throughput_chars_per_second']:.0f} å­—ç¬¦/ç§’")

def save_chunks_to_json(chunks: List[str], output_file: Path, source_file: str):
    """ä¿å­˜åˆ†å—ç»“æœä¸ºJSONæ ¼å¼"""
    import json
    
    result = {
        "source_file": source_file,
        "chunk_count": len(chunks),
        "chunks": [
            {
                "index": i,
                "content": chunk,
                "length": len(chunk)
            }
            for i, chunk in enumerate(chunks)
        ],
        "metadata": {
            "total_length": sum(len(chunk) for chunk in chunks),
            "avg_chunk_length": sum(len(chunk) for chunk in chunks) / len(chunks) if chunks else 0
        }
    }
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

# ä½¿ç”¨ç¤ºä¾‹
# process_document_batch("./documents", "./output/chunks")
```

### 4. è‡ªå®šä¹‰æ‰©å±•ç¤ºä¾‹

```python
# åˆ›å»ºä¸“é—¨çš„RAGæ–‡æ¡£å¤„ç†å™¨
class RAGDocumentProcessor(AstMarkdownSplitter):
    """ä¸“ä¸ºRAGåº”ç”¨ä¼˜åŒ–çš„æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, **kwargs):
        # RAGä¼˜åŒ–çš„é»˜è®¤é…ç½®
        rag_defaults = {
            'chunk_size': 512,           # é€‚åˆembeddingçš„å¤§å°
            'chunk_overlap': 100,        # ä¿è¯ä¸Šä¸‹æ–‡è¿ç»­æ€§
            'convert_table_ratio': 0.7,  # æ›´å¤šåœ°ä¿ç•™è¡¨æ ¼ç»“æ„
            'enable_first_line_as_title': False # ä¸å°†ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
        }
        rag_defaults.update(kwargs)
        super().__init__(**rag_defaults)
    
    def process_for_rag(self, text: str, metadata: dict = None) -> List[dict]:
        """å¤„ç†æ–‡æ¡£å¹¶è¿”å›RAGå‹å¥½çš„æ ¼å¼"""
        # åŸºç¡€åˆ†å—
        if metadata:
            metadata_str = self._format_metadata(metadata)
            chunks = self.split_text_metadata_aware(text, metadata_str)
        else:
            chunks = self.split_text(text)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ RAGç›¸å…³ä¿¡æ¯
        rag_chunks = []
        for i, chunk in enumerate(chunks):
            rag_chunk = {
                'id': f"chunk_{i:04d}",
                'content': chunk,
                'metadata': metadata or {},
                'chunk_index': i,
                'total_chunks': len(chunks),
                'char_count': len(chunk),
                'estimated_tokens': len(self._tokenizer(chunk)),
                'content_type': self._analyze_content_type(chunk)
            }
            rag_chunks.append(rag_chunk)
        
        return rag_chunks
    
    def _format_metadata(self, metadata: dict) -> str:
        """æ ¼å¼åŒ–å…ƒæ•°æ®ä¸ºå­—ç¬¦ä¸²"""
        return " | ".join([f"{k}: {v}" for k, v in metadata.items()])
    
    def _analyze_content_type(self, chunk: str) -> str:
        """åˆ†æå—å†…å®¹ç±»å‹"""
        if '|' in chunk and '---' in chunk:
            return 'table'
        elif chunk.startswith('#'):
            return 'heading'
        elif '1.' in chunk or '- ' in chunk:
            return 'list'
        else:
            return 'paragraph'

# ä½¿ç”¨RAGå¤„ç†å™¨
rag_processor = RAGDocumentProcessor()
rag_chunks = rag_processor.process_for_rag(
    document, 
    metadata={"source": "AIæŠ€æœ¯æŠ¥å‘Š", "domain": "äººå·¥æ™ºèƒ½"}
)

print(f"RAGå¤„ç†ç»“æœ: {len(rag_chunks)} ä¸ªå¢å¼ºå—")
for chunk in rag_chunks[:2]:  # æ˜¾ç¤ºå‰ä¸¤ä¸ªå—
    print(f"\nå—ID: {chunk['id']}")
    print(f"å†…å®¹ç±»å‹: {chunk['content_type']}")
    print(f"é¢„ä¼°tokens: {chunk['estimated_tokens']}")
    print(f"å†…å®¹é¢„è§ˆ: {chunk['content'][:100]}...")
```

## ğŸ¨ å®é™…åº”ç”¨åœºæ™¯

### 1. RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿ

```python
# RAGå‘é‡æ•°æ®åº“æ„å»º
from chunking import AstMarkdownSplitter
import chromadb
from sentence_transformers import SentenceTransformer

def build_rag_knowledge_base(documents: List[str], collection_name: str):
    """æ„å»ºRAGçŸ¥è¯†åº“"""
    # åˆå§‹åŒ–ç»„ä»¶
    splitter = AstMarkdownSplitter(chunk_size=512, chunk_overlap=50)
    embedding_model = SentenceTransformer('BAAI/bge-m3')
    client = chromadb.Client()
    collection = client.create_collection(collection_name)
    
    # å¤„ç†æ–‡æ¡£
    all_chunks = []
    all_metadata = []
    
    for doc_id, document in enumerate(documents):
        chunks = splitter.split_text(document)
        
        for chunk_id, chunk in enumerate(chunks):
            all_chunks.append(chunk)
            all_metadata.append({
                "doc_id": doc_id,
                "chunk_id": chunk_id,
                "chunk_size": len(chunk),
                "content_type": _analyze_content_type(chunk)
            })
    
    # ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨
    embeddings = embedding_model.encode(all_chunks)
    collection.add(
        embeddings=embeddings.tolist(),
        documents=all_chunks,
        metadatas=all_metadata,
        ids=[f"doc_{m['doc_id']}_chunk_{m['chunk_id']}" for m in all_metadata]
    )
    
    return collection

def _analyze_content_type(chunk: str) -> str:
    """åˆ†æå—å†…å®¹ç±»å‹"""
    if chunk.startswith("#"):
        return "heading"
    elif "|" in chunk and "---" in chunk:
        return "table"
    elif chunk.startswith(("- ", "1. ", "* ")):
        return "list"
    else:
        return "paragraph"
```

### 2. ä¼ä¸šçŸ¥è¯†åº“ç®¡ç†

```python
class EnterpriseKnowledgeProcessor:
    """ä¼ä¸šçŸ¥è¯†åº“å¤„ç†å™¨"""
    
    def __init__(self):
        self.splitter = AstMarkdownSplitter(
            chunk_size=800,
            chunk_overlap=100,
            convert_table_ratio=0.3  # ä¿ç•™æ›´å¤šè¡¨æ ¼ç»“æ„
        )
    
    def process_technical_documentation(self, docs_path: str) -> Dict:
        """å¤„ç†æŠ€æœ¯æ–‡æ¡£"""
        results = {
            "processed_files": 0,
            "total_chunks": 0,
            "content_types": {},
            "avg_chunk_size": 0,
            "quality_score": 0
        }
        
        for doc_file in Path(docs_path).glob("**/*.md"):
            with open(doc_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ–‡æ¡£å…ƒæ•°æ®
            metadata = self._extract_document_metadata(doc_file, content)
            
            # æ™ºèƒ½åˆ†å—
            chunks = self.splitter.split_text_metadata_aware(content, metadata)
            
            # åˆ†æå†…å®¹ç±»å‹
            for chunk in chunks:
                content_type = self._analyze_content_type(chunk)
                results["content_types"][content_type] = \
                    results["content_types"].get(content_type, 0) + 1
            
            results["processed_files"] += 1
            results["total_chunks"] += len(chunks)
            
            # ä¿å­˜ç»“æ„åŒ–ç»“æœ
            self._save_structured_chunks(doc_file, chunks, metadata)
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        results["avg_chunk_size"] = results["total_chunks"] / results["processed_files"]
        results["quality_score"] = self._calculate_quality_score(results)
        
        return results
    
    def _extract_document_metadata(self, file_path: Path, content: str) -> str:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            "filename": file_path.name,
            "path": str(file_path.parent),
            "size": len(content),
            "modified": file_path.stat().st_mtime,
            "sections": content.count("#"),
            "tables": content.count("|"),
            "lists": content.count("- ")
        }
        return " | ".join([f"{k}: {v}" for k, v in metadata.items()])
```

### 3. å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®å‡†å¤‡

```python
class LLMDataPreprocessor:
    """å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, target_context_length: int = 2048):
        self.target_length = target_context_length
        self.splitter = AstMarkdownSplitter(
            chunk_size=target_context_length,
            chunk_overlap=200,
            convert_table_ratio=0.8  # ä¿æŒæ›´å¤šåŸå§‹æ ¼å¼
        )
    
    def prepare_training_data(self, corpus_path: str, output_path: str):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        training_examples = []
        quality_stats = {
            "total_documents": 0,
            "total_chunks": 0,
            "avg_quality": 0,
            "filtered_chunks": 0
        }
        
        for doc_file in Path(corpus_path).glob("**/*.md"):
            with open(doc_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # é¢„å¤„ç†å’Œæ¸…ç†
            cleaned_content = self._preprocess_content(content)
            
            # æ™ºèƒ½åˆ†å—
            chunks = self.splitter.split_text(cleaned_content)
            
            # è´¨é‡è¿‡æ»¤
            high_quality_chunks = []
            for chunk in chunks:
                quality_score = self._assess_chunk_quality(chunk)
                if quality_score > 0.7:  # åªä¿ç•™é«˜è´¨é‡å—
                    high_quality_chunks.append({
                        "text": chunk,
                        "quality": quality_score,
                        "source": str(doc_file),
                        "length": len(chunk)
                    })
                else:
                    quality_stats["filtered_chunks"] += 1
            
            training_examples.extend(high_quality_chunks)
            quality_stats["total_documents"] += 1
            quality_stats["total_chunks"] += len(chunks)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        self._save_training_data(training_examples, output_path)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        quality_stats["avg_quality"] = sum(ex["quality"] for ex in training_examples) / len(training_examples)
        
        return quality_stats
    
    def _assess_chunk_quality(self, chunk: str) -> float:
        """è¯„ä¼°å—è´¨é‡"""
        factors = {
            "length_score": min(1.0, len(chunk) / self.target_length),
            "structure_score": self._assess_structure(chunk),
            "content_density": self._assess_content_density(chunk),
            "language_quality": self._assess_language_quality(chunk)
        }
        return sum(factors.values()) / len(factors)
```

### 4. å¤šè¯­è¨€æ–‡æ¡£å¤„ç†

```python
class MultilingualDocumentProcessor:
    """å¤šè¯­è¨€æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        # é’ˆå¯¹ä¸åŒè¯­è¨€çš„é…ç½®
        self.language_configs = {
            "zh": {
                "chunk_size": 512,
                "separators": ["ã€‚", "ï¼Ÿ", "ï¼", "ï¼›", "â€¦â€¦"],
                "tokenizer": self._create_chinese_tokenizer()
            },
            "en": {
                "chunk_size": 1024,
                "separators": [".", "?", "!", ";", "\n\n"],
                "tokenizer": self._create_english_tokenizer()
            },
            "ja": {
                "chunk_size": 768,
                "separators": ["ã€‚", "ï¼Ÿ", "ï¼", "â€¦"],
                "tokenizer": self._create_japanese_tokenizer()
            }
        }
    
    def process_multilingual_corpus(self, documents: Dict[str, str]) -> Dict:
        """å¤„ç†å¤šè¯­è¨€è¯­æ–™åº“"""
        results = {}
        
        for lang_code, content in documents.items():
            if lang_code not in self.language_configs:
                continue
            
            config = self.language_configs[lang_code]
            splitter = AstMarkdownSplitter(
                chunk_size=config["chunk_size"],
                tokenizer=config["tokenizer"]
            )
            
            chunks = splitter.split_text(content)
            results[lang_code] = {
                "chunks": chunks,
                "chunk_count": len(chunks),
                "avg_length": sum(len(c) for c in chunks) / len(chunks),
                "language": lang_code
            }
        
        return results
```

---

## ğŸ“– APIå‚è€ƒæ–‡æ¡£

### æ ¸å¿ƒç±»

#### AstMarkdownSplitter

**ç±»å®šä¹‰:**
```python
class AstMarkdownSplitter(MetadataAwareTextSplitter):
    def __init__(
        self,
        chunk_size: int = 1024,
        chunk_overlap: int = 20,
        tokenizer: Optional[Callable[[str], Sequence]] = None,
        convert_table_ratio: float = 0.5,
        enable_first_line_as_title: bool = True
    )
```

**å‚æ•°è¯´æ˜:**

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `chunk_size` | `int` | `1024` | æ¯ä¸ªå—çš„æœ€å¤§tokenæ•°é‡ |
| `chunk_overlap` | `int` | `20` | å—é—´é‡å çš„tokenæ•°é‡ |
| `tokenizer` | `Callable` | `None` | è‡ªå®šä¹‰åˆ†è¯å™¨å‡½æ•° |
| `convert_table_ratio` | `float` | `0.5` | è¡¨æ ¼è½¬æ¢ä¸ºæ®µè½çš„é˜ˆå€¼ï¼ˆ0-1ï¼‰ |
| `enable_first_line_as_title` | `bool` | `True` | æ˜¯å¦å°†é¦–è¡Œä½œä¸ºæ ‡é¢˜å¤„ç† |
| `min_chunk_tokens` | `int` | `50` | æœ€å°å—å¤§å°ï¼Œå°äºæ­¤å€¼çš„å—ä¼šè¢«åˆå¹¶ |

**ä¸»è¦æ–¹æ³•:**

##### `split_text(text: str) -> List[str]`
åˆ†å‰²æ–‡æœ¬ä¸ºå¤šä¸ªå—ã€‚

**å‚æ•°:**
- `text` (str): è¦åˆ†å‰²çš„Markdownæ–‡æœ¬

**è¿”å›:**
- `List[str]`: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨

**å¼‚å¸¸:**
- `TypeError`: å½“è¾“å…¥ä¸æ˜¯å­—ç¬¦ä¸²æ—¶
- `ChunkingError`: å½“åˆ†å‰²è¿‡ç¨‹å¤±è´¥æ—¶

**ç¤ºä¾‹:**
```python
splitter = AstMarkdownSplitter(chunk_size=512)
chunks = splitter.split_text("# æ ‡é¢˜\nå†…å®¹...")
```

##### `split_text_metadata_aware(text: str, metadata_str: str) -> List[str]`
å¸¦å…ƒæ•°æ®æ„ŸçŸ¥çš„æ–‡æœ¬åˆ†å‰²ã€‚

**å‚æ•°:**
- `text` (str): è¦åˆ†å‰²çš„æ–‡æœ¬
- `metadata_str` (str): å…ƒæ•°æ®å­—ç¬¦ä¸²

**è¿”å›:**
- `List[str]`: è€ƒè™‘å…ƒæ•°æ®ç©ºé—´çš„æ–‡æœ¬å—åˆ—è¡¨

**å¼‚å¸¸:**
- `ValueError`: å½“å…ƒæ•°æ®è¿‡é•¿æ—¶
- `TokenizationError`: å½“åˆ†è¯å¤±è´¥æ—¶

**ç¤ºä¾‹:**
```python
metadata = "author: å¼ ä¸‰ | date: 2024-01-15"
chunks = splitter.split_text_metadata_aware(text, metadata)
```

### é…ç½®ç±»

#### ChunkingConfig

**ç±»å®šä¹‰:**
```python
@dataclass
class ChunkingConfig:
    default_chunk_size: int = 1024
    default_chunk_overlap: int = 20
    min_effective_chunk_size: int = 50
    max_header_to_row_ratio: float = 2.0
    default_convert_table_ratio: float = 0.5
    sentence_separators: List[str] = None
    html_tags: List[str] = None
    log_level: str = "INFO"
```

**é…ç½®å‚æ•°è¯¦è§£:**

| å‚æ•° | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|------|----------|
| `default_chunk_size` | é»˜è®¤å—å¤§å° | æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦è°ƒæ•´ |
| `default_chunk_overlap` | é»˜è®¤é‡å å¤§å° | ä¸€èˆ¬è®¾ä¸ºchunk_sizeçš„5-10% |
| `min_effective_chunk_size` | æœ€å°æœ‰æ•ˆå—å¤§å° | é¿å…è¿‡å°çš„æ— æ„ä¹‰å— |
| `min_chunk_tokens` | æœ€å°å—tokenæ•° | å°äºæ­¤å€¼çš„å—ä¼šè¢«åˆå¹¶ |
| `max_header_to_row_ratio` | è¡¨å¤´è¡Œå¤§å°æ¯”ä¾‹é˜ˆå€¼ | æ§åˆ¶è¡¨æ ¼å¤„ç†ç­–ç•¥ |
| `sentence_separators` | å¥å­åˆ†éš”ç¬¦ | æ ¹æ®è¯­è¨€ç‰¹ç‚¹è‡ªå®šä¹‰ |

### å·¥å…·å‡½æ•°

#### `setup_logging(level: str, format_string: str) -> None`
è®¾ç½®æ—¥å¿—é…ç½®ã€‚

#### `estimate_tokens(text: str, tokenizer: Callable) -> int`
ä¼°ç®—æ–‡æœ¬tokenæ•°é‡ã€‚

#### `format_chunk_stats(chunks: List[str]) -> str`
æ ¼å¼åŒ–å—ç»Ÿè®¡ä¿¡æ¯ã€‚

### å¼‚å¸¸ç±»

```python
class ChunkingError(Exception):
    """åˆ†å—è¿‡ç¨‹çš„åŸºç¡€å¼‚å¸¸"""

class InvalidConfigurationError(ChunkingError):
    """é…ç½®å‚æ•°æ— æ•ˆ"""

class TokenizationError(ChunkingError):
    """åˆ†è¯è¿‡ç¨‹å¼‚å¸¸"""

class DocumentParsingError(ChunkingError):
    """æ–‡æ¡£è§£æå¼‚å¸¸"""

class HtmlProcessingError(ChunkingError):
    """HTMLå¤„ç†å¼‚å¸¸"""
```

---

## âš™ï¸ æ€§èƒ½è°ƒä¼˜æŒ‡å—

### 1. é’ˆå¯¹ä¸åŒæ–‡æ¡£ç±»å‹çš„è°ƒä¼˜

#### ğŸ“Š è¡¨æ ¼å¯†é›†å‹æ–‡æ¡£
```python
# æ¨èé…ç½®
table_heavy_config = AstMarkdownSplitter(
    chunk_size=800,              # é€‚ä¸­çš„å—å¤§å°
    chunk_overlap=60,            # è¾ƒå°çš„é‡å 
    convert_table_ratio=0.3,     # æ›´å®¹æ˜“è½¬æ¢è¡¨æ ¼ä¸ºæ®µè½
    enable_first_line_as_title=False
)

# é€‚ç”¨åœºæ™¯ï¼šæŠ€æœ¯è§„èŒƒã€æ•°æ®æŠ¥å‘Šã€APIæ–‡æ¡£
```

#### ğŸ“ æ–‡æœ¬å¯†é›†å‹æ–‡æ¡£
```python
# æ¨èé…ç½®
text_heavy_config = AstMarkdownSplitter(
    chunk_size=1200,             # è¾ƒå¤§çš„å—å¤§å°
    chunk_overlap=100,           # è¾ƒå¤§çš„é‡å ä¿è¯è¿è´¯æ€§
    convert_table_ratio=0.7,     # ä¿æŒè¡¨æ ¼ç»“æ„
    enable_first_line_as_title=True
)

# é€‚ç”¨åœºæ™¯ï¼šæŠ€æœ¯æ–‡ç« ã€æ•™ç¨‹ã€æ‰‹å†Œ
```

#### ğŸ“‹ åˆ—è¡¨å¯†é›†å‹æ–‡æ¡£
```python
# æ¨èé…ç½®
list_heavy_config = AstMarkdownSplitter(
    chunk_size=600,              # è¾ƒå°çš„å—é¿å…åˆ—è¡¨åˆ†å‰²
    chunk_overlap=30,            # è¾ƒå°çš„é‡å 
    convert_table_ratio=0.5,
    enable_first_line_as_title=True
)

# é€‚ç”¨åœºæ™¯ï¼šæ¸…å•ã€ç›®å½•ã€é…ç½®æ–‡æ¡£
```

### 2. ä¸åŒåº”ç”¨åœºæ™¯çš„è°ƒä¼˜

#### ğŸ” RAGæ£€ç´¢ä¼˜åŒ–
```python
# ä¼˜åŒ–æ£€ç´¢æ•ˆæœçš„é…ç½®
rag_optimized_config = AstMarkdownSplitter(
    chunk_size=512,              # é€‚åˆembeddingæ¨¡å‹
    chunk_overlap=100,           # ä¿è¯ä¸Šä¸‹æ–‡è¿ç»­æ€§
    convert_table_ratio=0.4,     # å°†å¤æ‚è¡¨æ ¼è½¬ä¸ºæ–‡æœ¬
    enable_first_line_as_title=False,
    min_chunk_tokens=30          # RAGä¸­è¾ƒå°çš„æœ€å°å€¼ä»¥ä¿ç•™æ›´å¤šç»†ç²’åº¦ä¿¡æ¯
)

# é¢å¤–ä¼˜åŒ–æŠ€å·§
def rag_post_processing(chunks: List[str]) -> List[str]:
    """RAGåå¤„ç†ä¼˜åŒ–"""
    optimized_chunks = []
    
    for chunk in chunks:
        # 1. è¿‡æ»¤è¿‡çŸ­çš„å—
        if len(chunk.strip()) < 50:
            continue
        
        # 2. æ ‡å‡†åŒ–æ ¼å¼
        chunk = standardize_chunk_format(chunk)
        
        # 3. æ·»åŠ ä¸Šä¸‹æ–‡æ ‡è¯†
        chunk = add_context_markers(chunk)
        
        optimized_chunks.append(chunk)
    
    return optimized_chunks
```

#### ğŸ¤– å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–
```python
# è®­ç»ƒæ•°æ®å‡†å¤‡çš„é…ç½®
training_optimized_config = AstMarkdownSplitter(
    chunk_size=2048,             # åŒ¹é…æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦
    chunk_overlap=200,           # é€‚åº¦é‡å 
    convert_table_ratio=0.8,     # ä¿æŒåŸå§‹æ ¼å¼
    enable_first_line_as_title=True
)
```

### 3. æ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜

#### å†…å­˜ä½¿ç”¨ç›‘æ§
```python
import psutil
import gc
from chunking import metrics

def monitor_memory_usage(splitter, documents):
    """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process()
    
    # è®°å½•åˆå§‹å†…å­˜
    initial_memory = process.memory_info().rss / 1024 / 1024
    
    results = []
    for i, doc in enumerate(documents):
        # å¤„ç†å‰å†…å­˜
        before_memory = process.memory_info().rss / 1024 / 1024
        
        # æ‰§è¡Œåˆ†å—
        chunks = splitter.split_text(doc)
        
        # å¤„ç†åå†…å­˜
        after_memory = process.memory_info().rss / 1024 / 1024
        
        results.append({
            "doc_index": i,
            "doc_size": len(doc),
            "chunk_count": len(chunks),
            "memory_delta": after_memory - before_memory,
            "memory_efficiency": len(doc) / (after_memory - before_memory + 0.001)
        })
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        if i % 10 == 0:
            gc.collect()
    
    return results
```

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
def performance_benchmark(configurations, test_documents):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    benchmark_results = {}
    
    for config_name, config in configurations.items():
        splitter = AstMarkdownSplitter(**config)
        
        start_time = time.time()
        total_chunks = 0
        total_chars = 0
        
        for doc in test_documents:
            chunks = splitter.split_text(doc)
            total_chunks += len(chunks)
            total_chars += len(doc)
        
        end_time = time.time()
        duration = end_time - start_time
        
        benchmark_results[config_name] = {
            "duration": duration,
            "throughput_chars_per_sec": total_chars / duration,
            "throughput_docs_per_sec": len(test_documents) / duration,
            "avg_chunks_per_doc": total_chunks / len(test_documents),
            "processing_speed": f"{total_chars / duration:.0f} chars/sec"
        }
    
    return benchmark_results

# ä½¿ç”¨ç¤ºä¾‹
configurations = {
    "conservative": {"chunk_size": 512, "chunk_overlap": 50},
    "balanced": {"chunk_size": 1024, "chunk_overlap": 100},
    "aggressive": {"chunk_size": 2048, "chunk_overlap": 200}
}

results = performance_benchmark(configurations, test_documents)
```

### 4. å¸¸è§æ€§èƒ½é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜1: å†…å­˜ä½¿ç”¨è¿‡é«˜
**ç—‡çŠ¶**: å¤„ç†å¤§æ–‡æ¡£æ—¶å†…å­˜æŒç»­å¢é•¿
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¯ç”¨ç¼“å­˜æ¸…ç†
splitter._cache_manager.clear_all()

# 2. åˆ†æ‰¹å¤„ç†
def batch_process_large_documents(documents, batch_size=10):
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        # å¤„ç†æ‰¹æ¬¡
        yield process_batch(batch)
```

#### é—®é¢˜2: å¤„ç†é€Ÿåº¦æ…¢
**ç—‡çŠ¶**: å¤„ç†é€Ÿåº¦æ˜æ˜¾ä½äºé¢„æœŸ
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä¼˜åŒ–tokenizeré€‰æ‹©
import tiktoken
fast_tokenizer = tiktoken.get_encoding("cl100k_base").encode

# 2. å‡å°‘ASTè§£ææ·±åº¦
splitter = AstMarkdownSplitter(
    chunk_size=1024,
    convert_table_ratio=0.3  # é™ä½è¡¨æ ¼å¤„ç†å¤æ‚åº¦
)

# 3. é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
import re
SENTENCE_PATTERN = re.compile(r'[.!?ã€‚ï¼ï¼Ÿ]')
```

#### é—®é¢˜3: åˆ†å—è´¨é‡ä¸ä½³
**ç—‡çŠ¶**: åœ¨å¥å­ä¸­é—´åˆ‡æ–­ï¼Œè¯­ä¹‰ä¸å®Œæ•´
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´å—å¤§å°å’Œé‡å 
splitter = AstMarkdownSplitter(
    chunk_size=800,      # ä»1024å‡å°‘åˆ°800
    chunk_overlap=80
)

# 2. è‡ªå®šä¹‰å¥å­åˆ†éš”ç¬¦
CONFIG.sentence_separators = [
    "ã€‚", "ï¼Ÿ", "ï¼", "ï¼›",  # ä¸­æ–‡
    ".", "?", "!", ";",     # è‹±æ–‡
    "\n\n",                 # æ®µè½
    "â€¦â€¦", "â€¦"              # çœç•¥å·
]

# 3. åå¤„ç†éªŒè¯
def fix_broken_sentences(chunks):
    fixed_chunks = []
    for i, chunk in enumerate(chunks):
        # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç»“æŸ
        if not chunk.rstrip().endswith(("ã€‚", ".", "!", "?")):
            # å°è¯•ä»ä¸‹ä¸€ä¸ªå—å€Ÿç”¨å†…å®¹
            if i + 1 < len(chunks):
                next_chunk = chunks[i + 1]
                sentences = next_chunk.split("ã€‚")
                if len(sentences) > 1:
                    chunk += sentences[0] + "ã€‚"
                    chunks[i + 1] = "ã€‚".join(sentences[1:])
        fixed_chunks.append(chunk)
    return fixed_chunks
```

**Q: è¡¨æ ¼è¢«ç ´åäº†å¦‚ä½•å¤„ç†ï¼Ÿ**

A: è¡¨æ ¼å¤„ç†ç­–ç•¥ï¼š
```python
# 1. æé«˜convert_table_ratioä¿æŒè¡¨æ ¼ç»“æ„
splitter = AstMarkdownSplitter(
    convert_table_ratio=0.8  # æ›´éš¾è½¬æ¢ä¸ºæ®µè½
)

# 2. è‡ªå®šä¹‰è¡¨æ ¼å¤„ç†
class TablePreservingSplitter(AstMarkdownSplitter):
    def _split_table(self, table, max_size):
        # å¼ºåˆ¶ä¿æŒè¡¨æ ¼å®Œæ•´æ€§
        table_text = self._render_block(table)
        if len(self._tokenizer(table_text)) <= max_size:
            return [table]
        else:
            # è½¬æ¢ä¸ºæ®µè½ä½†ä¿æŒç»“æ„ä¿¡æ¯
            return self._convert_table_to_structured_text(table)

# 3. åå¤„ç†é‡å»ºè¡¨æ ¼
def restore_table_structure(chunks):
    for i, chunk in enumerate(chunks):
        if "è¡¨å¤´1:" in chunk and "è¡¨å¤´2:" in chunk:
            # æ£€æµ‹åˆ°è½¬æ¢çš„è¡¨æ ¼ï¼Œå°è¯•é‡å»º
            chunks[i] = rebuild_table_format(chunk)
    return chunks
```

### ğŸ”§ é›†æˆé—®é¢˜

**Q: å¦‚ä½•ä¸LangChainé›†æˆï¼Ÿ**

A: é›†æˆç¤ºä¾‹ï¼š
```python
from langchain.text_splitter import TextSplitter
from chunking import AstMarkdownSplitter

class LangChainASTSplitter(TextSplitter):
    def __init__(self, **kwargs):
        super().__init__()
        self.ast_splitter = AstMarkdownSplitter(**kwargs)
    
    def split_text(self, text: str) -> List[str]:
        return self.ast_splitter.split_text(text)

# ä½¿ç”¨
splitter = LangChainASTSplitter(chunk_size=1000)
docs = splitter.create_documents([text])
```

**Q: å¦‚ä½•ä¸å‘é‡æ•°æ®åº“é›†æˆï¼Ÿ**

A: å¤šç§å‘é‡æ•°æ®åº“é›†æˆï¼š
```python
# Chromaé›†æˆ
import chromadb
from sentence_transformers import SentenceTransformer

def integrate_with_chroma(documents):
    splitter = AstMarkdownSplitter(chunk_size=512)
    model = SentenceTransformer('BAAI/bge-m3')
    client = chromadb.Client()
    collection = client.create_collection("documents")
    
    for doc_id, doc in enumerate(documents):
        chunks = splitter.split_text(doc)
        embeddings = model.encode(chunks)
        
        collection.add(
            embeddings=embeddings.tolist(),
            documents=chunks,
            metadatas=[{"doc_id": doc_id, "chunk_id": i} 
                      for i in range(len(chunks))],
            ids=[f"doc_{doc_id}_chunk_{i}" for i in range(len(chunks))]
        )

# Weaviateé›†æˆ
import weaviate

def integrate_with_weaviate(documents):
    client = weaviate.Client("http://localhost:8080")
    splitter = AstMarkdownSplitter(chunk_size=768)
    
    for doc in documents:
        chunks = splitter.split_text(doc)
        for chunk in chunks:
            client.data_object.create(
                data_object={"content": chunk},
                class_name="Document"
            )
```

---

## â“ å¸¸è§é—®é¢˜FAQ

### ğŸ”§ å®‰è£…å’Œé…ç½®é—®é¢˜

**Q: å®‰è£…æ—¶å‡ºç° `ModuleNotFoundError: No module named 'mistletoe'` é”™è¯¯ï¼Ÿ**

A: è¿™æ˜¯ä¾èµ–åŒ…æœªå®‰è£…å¯¼è‡´çš„ã€‚è¯·æ‰§è¡Œï¼š
```bash
pip install mistletoe pydantic
# æˆ–è€…å¦‚æœæœ‰requirements.txt
pip install -r requirements.txt
```

**Q: å¦‚ä½•éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸï¼Ÿ**

A: è¿è¡Œä»¥ä¸‹ä»£ç è¿›è¡Œå¿«é€ŸéªŒè¯ï¼š
```python
try:
    from chunking import AstMarkdownSplitter
    splitter = AstMarkdownSplitter()
    result = splitter.split_text("# æµ‹è¯•\nè¿™æ˜¯æµ‹è¯•å†…å®¹ã€‚")
    print(f"âœ… å®‰è£…æˆåŠŸï¼Œç”Ÿæˆäº† {len(result)} ä¸ªå—")
except Exception as e:
    print(f"âŒ å®‰è£…æœ‰é—®é¢˜: {e}")
```

### ğŸ“Š ä½¿ç”¨å’Œé…ç½®é—®é¢˜

**Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ `chunk_size`ï¼Ÿ**

A: é€‰æ‹©ä¾æ®ï¼š
- **embeddingæ¨¡å‹**: é€šå¸¸512-1024
- **LLMä¸Šä¸‹æ–‡**: åŒ¹é…æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚GPT-4çš„8kï¼‰
- **æ–‡æ¡£å¤æ‚åº¦**: å¤æ‚æ–‡æ¡£ç”¨è¾ƒå°å€¼ï¼ˆ512-800ï¼‰
- **æ£€ç´¢ç²¾åº¦**: è¾ƒå°çš„å—æé«˜æ£€ç´¢ç²¾åº¦

```python
# ä¸åŒåœºæ™¯çš„æ¨èé…ç½®
scenarios = {
    "rag_embedding": {"chunk_size": 512, "chunk_overlap": 50},
    "llm_training": {"chunk_size": 2048, "chunk_overlap": 200},
    "search_index": {"chunk_size": 768, "chunk_overlap": 76},
    "summarization": {"chunk_size": 1536, "chunk_overlap": 150}
}
```

**Q: `chunk_overlap` è®¾ç½®å¤šå°‘åˆé€‚ï¼Ÿ**

A: ä¸€èˆ¬å»ºè®®ï¼š
- **æ ‡å‡†è®¾ç½®**: chunk_sizeçš„5-10%
- **é«˜è¿è´¯æ€§éœ€æ±‚**: chunk_sizeçš„10-15%
- **æ€§èƒ½ä¼˜å…ˆ**: chunk_sizeçš„3-5%

**Q: ä»€ä¹ˆæ—¶å€™éœ€è¦è°ƒæ•´ `convert_table_ratio`ï¼Ÿ**

A: è°ƒæ•´åœºæ™¯ï¼š
- **è¡¨æ ¼ä¿æŒå®Œæ•´**: è®¾ç½®ä¸º0.8-0.9
- **ä¾¿äºæ–‡æœ¬å¤„ç†**: è®¾ç½®ä¸º0.2-0.4
- **å‡è¡¡å¤„ç†**: ä¿æŒé»˜è®¤0.5

**Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ `min_chunk_tokens` å€¼ï¼Ÿ**

A: é€‰æ‹©ä¾æ®ï¼š
- **RAGæ£€ç´¢**: 30-50ï¼ˆä¿ç•™ç»†ç²’åº¦ä¿¡æ¯ï¼Œæé«˜æ£€ç´¢ç²¾åº¦ï¼‰
- **LLMè®­ç»ƒ**: 100-200ï¼ˆé¿å…è¿‡å°çš„æ— æ„ä¹‰ç‰‡æ®µï¼‰
- **çŸ¥è¯†åº“æ„å»º**: 50-100ï¼ˆå¹³è¡¡ä¿¡æ¯å®Œæ•´æ€§å’Œå­˜å‚¨æ•ˆç‡ï¼‰
- **æœç´¢ç´¢å¼•**: 20-50ï¼ˆä¼˜åŒ–æœç´¢ç»“æœç›¸å…³æ€§ï¼‰

```python
# ä¸åŒåœºæ™¯çš„æ¨èé…ç½®
min_chunk_scenarios = {
    "rag_fine_grained": 30,      # ç»†ç²’åº¦æ£€ç´¢
    "rag_standard": 50,          # æ ‡å‡†RAGåº”ç”¨  
    "llm_training": 100,         # å¤§æ¨¡å‹è®­ç»ƒæ•°æ®
    "knowledge_base": 75,        # ä¼ä¸šçŸ¥è¯†åº“
    "search_engine": 40,         # æœç´¢å¼•æ“ç´¢å¼•
    "document_summary": 80       # æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ
}
```

**Q: `min_chunk_tokens` è®¾ç½®è¿‡å¤§æˆ–è¿‡å°ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ**

A: å½±å“åˆ†æï¼š

**è®¾ç½®è¿‡å°ï¼ˆ< 20ï¼‰**:
- âŒ äº§ç”Ÿå¤§é‡æ— æ„ä¹‰çš„å°å—ï¼ˆå¦‚å•ç‹¬çš„æ ‡é¢˜ï¼‰
- âŒ å¢åŠ å­˜å‚¨å’Œæ£€ç´¢å¼€é”€
- âŒ é™ä½æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§

**è®¾ç½®è¿‡å¤§ï¼ˆ> 200ï¼‰**:
- âŒ å¯èƒ½åˆå¹¶ä¸ç›¸å…³çš„å†…å®¹
- âŒ å‡å°‘åˆ†å—çš„ç»†ç²’åº¦
- âŒ å½±å“ç²¾ç¡®æ£€ç´¢çš„èƒ½åŠ›

**åˆç†èŒƒå›´ï¼ˆ30-100ï¼‰**:
- âœ… å¹³è¡¡å†…å®¹å®Œæ•´æ€§å’Œç»†ç²’åº¦
- âœ… ä¼˜åŒ–æ£€ç´¢è´¨é‡
- âœ… å‡å°‘å­˜å‚¨ç¢ç‰‡

### ğŸš€ æ€§èƒ½é—®é¢˜

**Q: å¤„ç†å¤§æ–‡æ¡£æ—¶å†…å­˜å ç”¨è¿‡é«˜æ€ä¹ˆåŠï¼Ÿ**

A: è§£å†³æ–¹æ¡ˆï¼š
```python
# 1. æµå¼å¤„ç†
def process_large_file_streaming(file_path):
    splitter = AstMarkdownSplitter(chunk_size=1024)
    
    with open(file_path, 'r') as f:
        buffer = []
        for line in f:
            buffer.append(line)
            if len(''.join(buffer)) > 10000:  # 10KBç¼“å†²
                text = ''.join(buffer)
                chunks = splitter.split_text(text)
                for chunk in chunks:
                    yield chunk
                buffer = []

# 2. å®šæœŸæ¸…ç†ç¼“å­˜
splitter._cache_manager.clear_all()

# 3. åˆ†æ‰¹å¤„ç†
def batch_process(documents, batch_size=5):
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        # å¤„ç†æ‰¹æ¬¡
        yield process_batch(batch)
```

**Q: å¤„ç†é€Ÿåº¦æ¯”è¾ƒæ…¢ï¼Œå¦‚ä½•ä¼˜åŒ–ï¼Ÿ**

A: ä¼˜åŒ–ç­–ç•¥ï¼š
```python
# 1. ä½¿ç”¨é«˜æ•ˆçš„tokenizer
import tiktoken
fast_tokenizer = tiktoken.get_encoding("cl100k_base")

splitter = AstMarkdownSplitter(
    tokenizer=fast_tokenizer.encode,
    chunk_size=1024
)

# 2. å¹¶è¡Œå¤„ç†ï¼ˆæ³¨æ„çº¿ç¨‹å®‰å…¨ï¼‰
from concurrent.futures import ThreadPoolExecutor

def parallel_split(documents):
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for doc in documents:
            future = executor.submit(splitter.split_text, doc)
            futures.append(future)
        
        results = []
        for future in futures:
            results.append(future.result())
        return results

# 3. é¢„å¤„ç†ä¼˜åŒ–
def preprocess_document(text):
    # ç§»é™¤ä¸å¿…è¦çš„ç©ºç™½è¡Œ
    lines = [line for line in text.split('\n') if line.strip()]
    return '\n'.join(lines)
```

### ğŸ¯ åˆ†å—è´¨é‡é—®é¢˜

**Q: åˆ†å—åœ¨å¥å­ä¸­é—´åˆ‡æ–­æ€ä¹ˆåŠï¼Ÿ**

A: è°ƒæ•´ç­–ç•¥ï¼š
```python
# 1. å‡å°chunk_sizeä¸ºåˆ†å‰²ç•™ç©ºé—´
splitter = AstMarkdownSplitter(
    chunk_size=800,  # ä»1024å‡å°‘åˆ°800
    chunk_overlap=80
)

# 2. è‡ªå®šä¹‰å¥å­åˆ†éš”ç¬¦
CONFIG.sentence_separators = [
    "ã€‚", "ï¼Ÿ", "ï¼", "ï¼›",  # ä¸­æ–‡
    ".", "?", "!", ";",     # è‹±æ–‡
    "\n\n",                 # æ®µè½
    "â€¦â€¦", "â€¦"              # çœç•¥å·
]

# 3. åå¤„ç†éªŒè¯
def fix_broken_sentences(chunks):
    fixed_chunks = []
    for i, chunk in enumerate(chunks):
        # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç»“æŸ
        if not chunk.rstrip().endswith(("ã€‚", ".", "!", "?")):
            # å°è¯•ä»ä¸‹ä¸€ä¸ªå—å€Ÿç”¨å†…å®¹
            if i + 1 < len(chunks):
                next_chunk = chunks[i + 1]
                sentences = next_chunk.split("ã€‚")
                if len(sentences) > 1:
                    chunk += sentences[0] + "ã€‚"
                    chunks[i + 1] = "ã€‚".join(sentences[1:])
        fixed_chunks.append(chunk)
    return fixed_chunks
```

### ğŸ”§ é›†æˆé—®é¢˜

**Q: å¦‚ä½•ä¸LangChainé›†æˆï¼Ÿ**

A: é›†æˆç¤ºä¾‹ï¼š
```python
from langchain.text_splitter import TextSplitter
from chunking import AstMarkdownSplitter

class LangChainASTSplitter(TextSplitter):
    def __init__(self, **kwargs):
        super().__init__()
        self.ast_splitter = AstMarkdownSplitter(**kwargs)
    
    def split_text(self, text: str) -> List[str]:
        return self.ast_splitter.split_text(text)

# ä½¿ç”¨
splitter = LangChainASTSplitter(chunk_size=1000)
docs = splitter.create_documents([text])
```

**Q: å¦‚ä½•ä¸å‘é‡æ•°æ®åº“é›†æˆï¼Ÿ**

A: å¤šç§å‘é‡æ•°æ®åº“é›†æˆï¼š
```python
# Chromaé›†æˆ
import chromadb
from sentence_transformers import SentenceTransformer

def integrate_with_chroma(documents):
    splitter = AstMarkdownSplitter(chunk_size=512)
    model = SentenceTransformer('BAAI/bge-m3')
    client = chromadb.Client()
    collection = client.create_collection("documents")
    
    for doc_id, doc in enumerate(documents):
        chunks = splitter.split_text(doc)
        embeddings = model.encode(chunks)
        
        collection.add(
            embeddings=embeddings.tolist(),
            documents=chunks,
            metadatas=[{"doc_id": doc_id, "chunk_id": i} 
                      for i in range(len(chunks))],
            ids=[f"doc_{doc_id}_chunk_{i}" for i in range(len(chunks))]
        )

# Weaviateé›†æˆ
import weaviate

def integrate_with_weaviate(documents):
    client = weaviate.Client("http://localhost:8080")
    splitter = AstMarkdownSplitter(chunk_size=768)
    
    for doc in documents:
        chunks = splitter.split_text(doc)
        for chunk in chunks:
            client.data_object.create(
                data_object={"content": chunk},
                class_name="Document"
            )
```

### ğŸ› é”™è¯¯å¤„ç†

**Q: é‡åˆ° `TokenizationError` æ€ä¹ˆåŠï¼Ÿ**

A: é”™è¯¯å¤„ç†ç­–ç•¥ï¼š
```python
from chunking import TokenizationError, AstMarkdownSplitter

def robust_splitting(text):
    splitter = AstMarkdownSplitter()
    
    try:
        return splitter.split_text(text)
    except TokenizationError as e:
        print(f"åˆ†è¯é”™è¯¯: {e}")
        # å›é€€åˆ°ç®€å•åˆ†å‰²
        return simple_text_split(text, 1000)
    except Exception as e:
        print(f"å…¶ä»–é”™è¯¯: {e}")
        return [text]  # è¿”å›åŸæ–‡

def simple_text_split(text, chunk_size):
    """ç®€å•çš„æ–‡æœ¬åˆ†å‰²å›é€€æ–¹æ¡ˆ"""
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i:i+chunk_size])
    return chunks
```

**Q: å¦‚ä½•å¤„ç†ç‰¹æ®Šå­—ç¬¦æˆ–ç¼–ç é—®é¢˜ï¼Ÿ**

A: æ–‡æœ¬é¢„å¤„ç†ï¼š
```python
import re
import unicodedata

def preprocess_text(text):
    # 1. æ ‡å‡†åŒ–Unicode
    text = unicodedata.normalize('NFKC', text)
    
    # 2. ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\u4e00-\u9fff\w\s\.,!?;:"""''()ã€ã€‘\[\]{}]', '', text)
    
    # 3. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    return text.strip()

# ä½¿ç”¨é¢„å¤„ç†
def safe_split_text(text):
    cleaned_text = preprocess_text(text)
    splitter = AstMarkdownSplitter()
    return splitter.split_text(cleaned_text)
```

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼ä»¥ä¸‹æ˜¯å‚ä¸é¡¹ç›®å¼€å‘çš„æŒ‡å—ã€‚

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# 1. Forké¡¹ç›®å¹¶å…‹éš†
git clone https://github.com/your-username/huawei-wl.git
cd huawei-wl/chunking

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# 4. å®‰è£…pre-commité’©å­
pre-commit install
```

### ä»£ç è§„èŒƒ

```python
# ä»£ç é£æ ¼ï¼šéµå¾ªPEP 8
# ä½¿ç”¨blackè¿›è¡Œæ ¼å¼åŒ–
black chunking/

# ä½¿ç”¨isortæ•´ç†å¯¼å…¥
isort chunking/

# ä½¿ç”¨flake8æ£€æŸ¥ä»£ç è´¨é‡
flake8 chunking/

# ä½¿ç”¨mypyè¿›è¡Œç±»å‹æ£€æŸ¥
mypy chunking/
```

### æµ‹è¯•æŒ‡å—

```python
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_splitter.py

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=chunking tests/

# æ€§èƒ½æµ‹è¯•
pytest tests/performance/
```

### æäº¤è§„èŒƒ

```bash
# æäº¤ä¿¡æ¯æ ¼å¼
feat: æ·»åŠ æ–°åŠŸèƒ½
fix: ä¿®å¤bug
docs: æ›´æ–°æ–‡æ¡£
style: ä»£ç æ ¼å¼è°ƒæ•´
refactor: é‡æ„
test: æ·»åŠ æµ‹è¯•
chore: å…¶ä»–æ‚é¡¹

# ç¤ºä¾‹
git commit -m "feat: æ·»åŠ è‡ªå®šä¹‰tokenizeræ”¯æŒ"
git commit -m "fix: ä¿®å¤è¡¨æ ¼åˆ†å‰²æ—¶çš„å†…å­˜æ³„æ¼"
git commit -m "docs: æ›´æ–°APIæ–‡æ¡£ä¸­çš„ç¤ºä¾‹ä»£ç "
```

### æ–°åŠŸèƒ½å¼€å‘æµç¨‹

1. **åˆ›å»ºIssue**: è¯¦ç»†æè¿°æ–°åŠŸèƒ½éœ€æ±‚
2. **åˆ›å»ºåˆ†æ”¯**: `git checkout -b feature/your-feature-name`
3. **å¼€å‘å®ç°**: éµå¾ªç°æœ‰ä»£ç é£æ ¼
4. **ç¼–å†™æµ‹è¯•**: ç¡®ä¿æ–°åŠŸèƒ½æœ‰å……åˆ†çš„æµ‹è¯•è¦†ç›–
5. **æ›´æ–°æ–‡æ¡£**: æ›´æ–°ç›¸å…³çš„APIæ–‡æ¡£å’Œç¤ºä¾‹
6. **æäº¤PR**: å¡«å†™è¯¦ç»†çš„PRæè¿°

### é‡è¦å¼€å‘åŸåˆ™

- **å‘åå…¼å®¹**: æ–°åŠŸèƒ½ä¸åº”ç ´åç°æœ‰API
- **æ€§èƒ½ä¼˜å…ˆ**: ç¡®ä¿æ–°åŠŸèƒ½ä¸æ˜¾è‘—å½±å“æ€§èƒ½
- **æµ‹è¯•è¦†ç›–**: æ‰€æœ‰æ–°ä»£ç éƒ½åº”æœ‰ç›¸åº”æµ‹è¯•
- **æ–‡æ¡£å®Œæ•´**: æ–°åŠŸèƒ½å¿…é¡»æœ‰å®Œæ•´çš„æ–‡æ¡£è¯´æ˜

---

## ğŸ“ˆ æŠ€æœ¯è·¯çº¿å›¾

### å½“å‰ç‰ˆæœ¬ (v1.0.0)
- âœ… åŸºç¡€ASTè§£æå’Œåˆ†å—
- âœ… å…ƒæ•°æ®æ„ŸçŸ¥åˆ†å—
- âœ… æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—
- âœ… é…ç½®ç®¡ç†ç³»ç»Ÿ
- âœ… é”™è¯¯å¤„ç†æœºåˆ¶

### è®¡åˆ’åŠŸèƒ½ (v1.1.0)
- ğŸ”„ å¤šè¯­è¨€æ”¯æŒä¼˜åŒ–
- ğŸ”„ æ›´å¤šæ–‡æ¡£æ ¼å¼æ”¯æŒï¼ˆLaTeX, reStructuredTextï¼‰
- ğŸ”„ æ™ºèƒ½é‡å ç­–ç•¥
- ğŸ”„ GPUåŠ é€Ÿåˆ†è¯

### æœªæ¥ç‰ˆæœ¬ (v2.0.0)
- ğŸ“‹ æœºå™¨å­¦ä¹ é©±åŠ¨çš„åˆ†å—ä¼˜åŒ–
- ğŸ“‹ å®æ—¶æµå¼å¤„ç†æ”¯æŒ
- ğŸ“‹ åˆ†å¸ƒå¼å¤„ç†èƒ½åŠ›
- ğŸ“‹ æ›´å¤šembeddingæ¨¡å‹é›†æˆ

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

**ğŸ‰ æ„Ÿè°¢ä½¿ç”¨ASTæ™ºèƒ½æ–‡æœ¬åˆ†å—åº“ï¼**

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª â­ Starï¼

æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ [Issue](https://github.com/Lauorie/chunking/issues) æˆ–å‚ä¸è®¨è®ºã€‚ 